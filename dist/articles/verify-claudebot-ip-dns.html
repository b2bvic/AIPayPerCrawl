<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity | AI Pay Per Crawl</title>
    <meta name="description" content="Technical guide to verifying ClaudeBot crawler authenticity through IP validation, DNS lookup, and preventing User-Agent spoofing attacks.">
    <meta name="author" content="Victor Valentine Romo">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity">
    <meta property="og:description" content="Technical guide to verifying ClaudeBot crawler authenticity through IP validation, DNS lookup, and preventing User-Agent spoofing attacks.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://aipaypercrawl.com/articles/verify-claudebot-ip-dns">
    <meta property="og:site_name" content="AI Pay Per Crawl">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity">
    <meta name="twitter:description" content="Technical guide to verifying ClaudeBot crawler authenticity through IP validation, DNS lookup, and preventing User-Agent spoofing attacks.">
    <link rel="canonical" href="https://aipaypercrawl.com/articles/verify-claudebot-ip-dns">
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "TechArticle",
  "headline": "Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity",
  "description": "Technical guide to verifying ClaudeBot crawler authenticity through IP validation, DNS lookup, and preventing User-Agent spoofing attacks.",
  "author": {
    "@type": "Person",
    "@id": "https://victorvalentineromo.com/#person",
    "name": "Victor Valentine Romo",
    "url": "https://victorvalentineromo.com"
  },
  "publisher": {
    "@type": "Organization",
    "@id": "https://aipaypercrawl.com/#organization",
    "name": "AI Pay Per Crawl",
    "url": "https://aipaypercrawl.com"
  },
  "datePublished": "2026-02-08",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aipaypercrawl.com/articles/verify-claudebot-ip-dns"
  }
}
    </script>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://aipaypercrawl.com"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Articles",
      "item": "https://aipaypercrawl.com/articles.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity",
      "item": "https://aipaypercrawl.com/articles/verify-claudebot-ip-dns"
    }
  ]
}
    </script>

    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect fill='%234f46e5' width='100' height='100' rx='12'/><text x='50' y='70' font-family='monospace' font-size='38' font-weight='700' fill='%23ffffff' text-anchor='middle'>APC</text></svg>">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet"></noscript>
    <link rel="stylesheet" href="/base.css">
    <link rel="me" href="https://scalewithsearch.com">
    <link rel="me" href="https://victorvalentineromo.com">
    <link rel="me" href="https://aifirstsearch.com">
    <link rel="me" href="https://browserprompt.com">
    <link rel="me" href="https://creatinepedia.com">
    <link rel="me" href="https://polytraffic.com">
    <link rel="me" href="https://tattooremovalnear.com">
    <link rel="me" href="https://comicstripai.com">
    <link rel="me" href="https://aipaypercrawl.com">
    <link rel="me" href="https://b2bvic.com">
    <link rel="me" href="https://seobyrole.com">
    <link rel="me" href="https://quickfixseo.com">
</head>
<body>


  <nav class="nav" role="navigation" aria-label="Primary">
    <div class="nav__inner">
      <a href="/" class="nav__logo">APC</a>

      <div class="nav__links">
        <!-- Implementation dropdown -->
        <div class="mega-wrapper" data-mega="implementation">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-implementation">
            Implementation
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Pricing dropdown -->
        <div class="mega-wrapper" data-mega="pricing">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-pricing">
            Pricing
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Crawlers dropdown -->
        <div class="mega-wrapper" data-mega="crawlers">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-crawlers">
            Crawlers
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Legal (direct link) -->
        <a href="/articles.html" class="nav__link">Legal</a>

        <div class="nav__divider"></div>

        <a href="/setup.html" class="nav__cta">Master the Protocol &mdash; $2,497</a>
      </div>

      <button class="nav__mobile-btn" id="mobile-menu-btn" aria-label="Open menu">
        <svg id="menu-icon" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
        <svg id="close-icon" class="hidden" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
      </button>
    </div>

    <!-- Mega Panel: Implementation -->
    <div id="mega-implementation" class="mega-panel" role="region" aria-label="Implementation guides">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Infrastructure</div>
          <a href="/articles.html" class="mega-link">Cloudflare Setup</a>
          <a href="/articles.html" class="mega-link">Nginx Rules</a>
          <a href="/articles.html" class="mega-link">Apache Config</a>
          <a href="/articles.html" class="mega-link">WordPress Plugin</a>
          <a href="/articles.html" class="mega-link">CDN Integration</a>
        </div>
        <div>
          <div class="mega-column__label">Protocols</div>
          <a href="/articles.html" class="mega-link">RSL Protocol</a>
          <a href="/articles.html" class="mega-link">llms.txt Specification</a>
          <a href="/articles.html" class="mega-link">robots.txt for AI</a>
          <a href="/articles.html" class="mega-link">Machine-Readable Terms</a>
        </div>
        <div>
          <div class="mega-column__label">Quick Start</div>
          <div style="background: linear-gradient(135deg, rgba(16,185,129,0.06), rgba(79,70,229,0.04)); border: 1px solid rgba(16,185,129,0.2); border-radius: 12px; padding: 1.5rem;">
            <h3 style="font-family: var(--font-display); font-size: 1.125rem; font-weight: 600; color: var(--text-primary); margin-bottom: 0.5rem;">Skip the learning curve.</h3>
            <p style="font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 1rem;">Complete pay-per-crawl implementation. Templates, pricing, contracts.</p>
            <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
          </div>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Pricing -->
    <div id="mega-pricing" class="mega-panel" role="region" aria-label="Pricing models">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Models</div>
          <a href="/articles.html" class="mega-link">Per-Crawl Pricing</a>
          <a href="/articles.html" class="mega-link">Flat-Rate Annual</a>
          <a href="/articles.html" class="mega-link">Tiered Pricing</a>
          <a href="/articles.html" class="mega-link">Volume Discounts</a>
          <a href="/articles.html" class="mega-link">Hybrid Models</a>
        </div>
        <div>
          <div class="mega-column__label">Benchmarks</div>
          <a href="/articles.html" class="mega-link">Rate Cards by Content Type</a>
          <a href="/articles.html" class="mega-link">Revenue Calculators</a>
          <a href="/articles.html" class="mega-link">Industry Benchmarks</a>
          <a href="/articles.html" class="mega-link">Deal Comparisons</a>
        </div>
        <div>
          <div class="mega-column__label">Recent Deals</div>
          <a href="/articles.html" class="mega-link">News Corp &times; OpenAI ($250M)</a>
          <a href="/articles.html" class="mega-link">Reddit &times; Google ($60M/yr)</a>
          <a href="/articles.html" class="mega-link">FT &times; Anthropic</a>
          <a href="/articles.html" class="mega-link">AP &times; OpenAI</a>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Crawlers -->
    <div id="mega-crawlers" class="mega-panel" role="region" aria-label="Crawler information">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Major Crawlers</div>
          <a href="/articles.html" class="mega-link">GPTBot (OpenAI)</a>
          <a href="/articles.html" class="mega-link">ClaudeBot (Anthropic)</a>
          <a href="/articles.html" class="mega-link">Googlebot-Extended</a>
          <a href="/articles.html" class="mega-link">Bytespider (ByteDance)</a>
          <a href="/articles.html" class="mega-link">CCBot (Common Crawl)</a>
        </div>
        <div>
          <div class="mega-column__label">Detection</div>
          <a href="/articles.html" class="mega-link">Bot Detection Methods</a>
          <a href="/articles.html" class="mega-link">User Agent Reference</a>
          <a href="/articles.html" class="mega-link">IP Range Verification</a>
          <a href="/articles.html" class="mega-link">Compliance Rates</a>
        </div>
        <div>
          <div class="mega-column__label">Directory</div>
          <a href="/articles.html" class="mega-link">Full Crawler Directory</a>
          <a href="/articles.html" class="mega-link">Crawler Behavior Matrix</a>
          <a href="/articles.html" class="mega-link">robots.txt Respect Rates</a>
        </div>
      </div>
    </div>

    <!-- Mobile Menu -->
    <div id="mobile-menu" class="mobile-menu">
      <a href="/articles.html" class="mobile-menu__link">Implementation</a>
      <a href="/articles.html" class="mobile-menu__link">Pricing</a>
      <a href="/articles.html" class="mobile-menu__link">Crawlers</a>
      <a href="/articles.html" class="mobile-menu__link">Legal</a>
      <a href="/articles.html" class="mobile-menu__link">All Articles</a>
      <a href="/setup.html" class="mobile-menu__cta">Master the Protocol &mdash; $2,497</a>
    </div>
  </nav>

  <main class="pt-nav">
    <div class="container--narrow" style="padding-top: var(--sp-8);">
      <div class="breadcrumbs">
        <a href="/">Home</a><span class="breadcrumbs__sep">/</span>
        <a href="/articles.html">Articles</a><span class="breadcrumbs__sep">/</span>
        <span>Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity</span>
      </div>

      <header style="margin-bottom: var(--sp-12);">
        <span class="label" style="margin-bottom: var(--sp-4); display: block;">Article &middot; 9 min read</span>
        <h1>Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity</h1>
        <p style="font-size: 1.125rem; color: var(--text-secondary); margin-top: var(--sp-4); max-width: 640px;">Technical guide to verifying ClaudeBot crawler authenticity through IP validation, DNS lookup, and preventing User-Agent spoofing attacks.</p>
      </header>

      <article class="article-body">
        <h1>Verify ClaudeBot IP and DNS: Authenticate Anthropic AI Crawler Identity</h1>
<p>Publishers implementing selective <strong>AI crawler</strong> policies must verify that requests claiming to be legitimate crawlers actually originate from declared organizations rather than malicious actors spoofing User-Agent strings. <strong>ClaudeBot</strong>, Anthropic&#39;s web crawler for training data collection, can be impersonated by anyone setting <code>User-Agent: ClaudeBot</code> in HTTP requests—making IP and DNS verification essential for distinguishing authentic Anthropic infrastructure from spoofed attempts to bypass crawler restrictions.</p>
<p><strong>DNS verification</strong> provides the most reliable authentication method for web crawlers. By performing reverse DNS lookups on request source IPs and validating that resulting hostnames match expected patterns (<code>*.anthropic.com</code>), then performing forward DNS resolution confirming hostnames resolve back to original IPs, publishers establish bidirectional verification that&#39;s cryptographically impractical to forge without compromising Anthropic&#39;s DNS infrastructure.</p>
<p>This verification matters because publishers often implement differentiated policies: allowing verified search crawlers for SEO while blocking AI training crawlers, or permitting licensed AI partners while blocking unauthorized training. Without verification, malicious scrapers easily claim to be Googlebot or ClaudeBot, evading restrictions designed to protect content. Robust authentication ensures technical controls function as intended rather than being trivially circumvented.</p>
<h2>ClaudeBot Identification Basics</h2>
<p>Before verifying authenticity, publishers must identify ClaudeBot among thousands of daily crawler requests through User-Agent inspection and behavioral patterns.</p>
<p><strong>User-Agent strings</strong> for ClaudeBot follow the pattern:</p>
<pre><code>Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; ClaudeBot/1.0; +https://www.anthropic.com)
</code></pre>
<p>Key characteristics include:</p>
<ul>
<li><strong>&quot;ClaudeBot&quot;</strong> identifier in the string</li>
<li><strong>Version number</strong> (1.0, though this may increment over time)</li>
<li><strong>Information URL</strong> pointing to Anthropic&#39;s documentation</li>
<li><strong>Mozilla compatibility</strong> declaration for web compatibility</li>
</ul>
<p>Simple detection through string matching:</p>
<pre><code class="language-python">def is_claudebot(user_agent: str) -&gt; bool:
    return &#39;claudebot&#39; in user_agent.lower()
</code></pre>
<p>However, this trivial check accepts any request claiming to be ClaudeBot, including spoofed attempts. Verification requires additional steps.</p>
<p><strong>Request patterns</strong> provide behavioral signals. Legitimate ClaudeBot typically exhibits:</p>
<ul>
<li><strong>Consistent User-Agent</strong>: Doesn&#39;t randomly vary across requests</li>
<li><strong>Respect for robots.txt</strong>: Honors disallow directives (when compliant)</li>
<li><strong>Reasonable rate limiting</strong>: Doesn&#39;t overwhelm servers with excessive requests</li>
<li><strong>Standard HTTP compliance</strong>: Follows redirects, respects cache headers</li>
</ul>
<p>Deviation from these patterns suggests problematic crawlers regardless of claimed identity.</p>
<p><strong>Public documentation</strong> from Anthropic describes ClaudeBot&#39;s purpose and behavior. As of early 2026, Anthropic provides:</p>
<ul>
<li>IP ranges for ClaudeBot infrastructure</li>
<li>robots.txt opt-out instructions</li>
<li>Contact information for crawler issues</li>
<li>Policy statements about data collection</li>
</ul>
<p>Publishers should monitor Anthropic&#39;s public documentation for updates to verification methods as infrastructure evolves.</p>
<h2>DNS Verification Process</h2>
<p>The gold standard for crawler authentication uses DNS bidirectional verification preventing IP spoofing and User-Agent forgery simultaneously.</p>
<p><strong>Reverse DNS lookup</strong> (rDNS) maps IP addresses to hostnames. When receiving a request claiming to be ClaudeBot:</p>
<ol>
<li>Extract the source IP address from the request</li>
<li>Perform reverse DNS query (PTR record lookup) on the IP</li>
<li>Obtain hostname (should be <code>*.anthropic.com</code> for legitimate ClaudeBot)</li>
<li>Verify hostname matches expected pattern</li>
</ol>
<p>Python implementation:</p>
<pre><code class="language-python">import socket

def reverse_dns_lookup(ip: str) -&gt; str:
    try:
        hostname = socket.gethostbyaddr(ip)[0]
        return hostname
    except socket.herror:
        return None
</code></pre>
<p>Example result for legitimate ClaudeBot: <code>crawler-123.anthropic.com</code></p>
<p><strong>Forward DNS verification</strong> confirms the hostname resolves back to the original IP:</p>
<pre><code class="language-python">def forward_dns_lookup(hostname: str) -&gt; list:
    try:
        return socket.gethostbyname_ex(hostname)[2]
    except socket.gaierror:
        return []
</code></pre>
<p><strong>Bidirectional verification</strong> combines both lookups:</p>
<pre><code class="language-python">def verify_claudebot(ip: str, user_agent: str) -&gt; bool:
    # First check User-Agent claims to be ClaudeBot
    if &#39;claudebot&#39; not in user_agent.lower():
        return False

    # Reverse DNS: IP -&gt; hostname
    hostname = reverse_dns_lookup(ip)
    if not hostname:
        return False

    # Verify hostname matches Anthropic pattern
    if not hostname.endswith(&#39;.anthropic.com&#39;):
        return False

    # Forward DNS: hostname -&gt; IPs
    resolved_ips = forward_dns_lookup(hostname)
    if not resolved_ips:
        return False

    # Confirm original IP is in resolved IPs
    return ip in resolved_ips
</code></pre>
<p>This process is cryptographically secure because attackers can&#39;t forge DNS records for domains they don&#39;t control. Even if they spoof source IPs (difficult with TCP), reverse DNS lookups would reveal actual domain ownership.</p>
<p><strong>Caching verification results</strong> improves performance:</p>
<pre><code class="language-python">from functools import lru_cache
import time

@lru_cache(maxsize=10000)
def cached_verify_claudebot(ip: str) -&gt; tuple:
    result = verify_claudebot_internal(ip)
    timestamp = time.time()
    return (result, timestamp)

def verify_with_cache(ip: str, user_agent: str) -&gt; bool:
    if &#39;claudebot&#39; not in user_agent.lower():
        return False

    result, timestamp = cached_verify_claudebot(ip)

    # Cache valid for 24 hours
    if time.time() - timestamp &gt; 86400:
        cached_verify_claudebot.cache_clear()
        result, _ = cached_verify_claudebot(ip)

    return result
</code></pre>
<p>Caching avoids repeated DNS lookups for frequently-accessing IPs, reducing latency and DNS query costs.</p>
<h2>IP Range Validation</h2>
<p>Anthropic publishes ClaudeBot IP ranges enabling direct IP-based verification without DNS lookups. This method is faster but requires maintaining updated range lists.</p>
<p><strong>Published IP ranges</strong> (example—check Anthropic documentation for current values):</p>
<pre><code>3.144.0.0/16
54.176.0.0/14
18.144.0.0/14
</code></pre>
<p><strong>CIDR range checking</strong> validates whether request IPs fall within published ranges:</p>
<pre><code class="language-python">import ipaddress

CLAUDEBOT_RANGES = [
    ipaddress.ip_network(&#39;3.144.0.0/16&#39;),
    ipaddress.ip_network(&#39;54.176.0.0/14&#39;),
    ipaddress.ip_network(&#39;18.144.0.0/14&#39;),
]

def is_ip_in_claudebot_ranges(ip_str: str) -&gt; bool:
    try:
        ip = ipaddress.ip_address(ip_str)
        return any(ip in network for network in CLAUDEBOT_RANGES)
    except ValueError:
        return False
</code></pre>
<p><strong>Automatic range updates</strong> fetch current ranges from Anthropic:</p>
<pre><code class="language-python">import requests
import json

def fetch_claudebot_ranges() -&gt; list:
    &quot;&quot;&quot;Fetch current IP ranges from Anthropic (hypothetical endpoint)&quot;&quot;&quot;
    try:
        response = requests.get(&#39;https://www.anthropic.com/claudebot-ranges.json&#39;, timeout=10)
        data = response.json()
        return [ipaddress.ip_network(cidr) for cidr in data[&#39;ranges&#39;]]
    except Exception as e:
        # Fall back to cached/default ranges
        return CLAUDEBOT_RANGES

# Refresh daily
RANGE_CACHE_DURATION = 86400
last_update = 0
current_ranges = CLAUDEBOT_RANGES

def get_current_ranges():
    global last_update, current_ranges
    if time.time() - last_update &gt; RANGE_CACHE_DURATION:
        current_ranges = fetch_claudebot_ranges()
        last_update = time.time()
    return current_ranges
</code></pre>
<p><strong>Combining DNS and IP verification</strong> provides defense-in-depth:</p>
<pre><code class="language-python">def verify_claudebot_comprehensive(ip: str, user_agent: str) -&gt; bool:
    if &#39;claudebot&#39; not in user_agent.lower():
        return False

    # Try IP range check first (faster)
    if is_ip_in_claudebot_ranges(ip):
        return True

    # Fall back to DNS verification
    return verify_claudebot_dns(ip)
</code></pre>
<p>This approach uses fast IP checking when possible, falling back to DNS for IPs outside known ranges (accounting for infrastructure changes).</p>
<h2>Implementation in Web Servers</h2>
<p>Integrating ClaudeBot verification into server configurations enables automatic policy enforcement without application-level changes.</p>
<p><strong>Nginx configuration</strong> using Lua module:</p>
<pre><code class="language-nginx">http {
    # Load Lua module
    lua_package_path &quot;/etc/nginx/lua/?.lua;;&quot;;

    # Define verification function
    init_by_lua_block {
        claudebot_verifier = require &quot;claudebot_verifier&quot;
    }

    server {
        location / {
            access_by_lua_block {
                local user_agent = ngx.var.http_user_agent or &quot;&quot;

                if string.match(user_agent:lower(), &quot;claudebot&quot;) then
                    local remote_ip = ngx.var.remote_addr
                    local verified = claudebot_verifier.verify(remote_ip)

                    if not verified then
                        ngx.status = 403
                        ngx.say(&quot;ClaudeBot verification failed&quot;)
                        return ngx.exit(403)
                    end
                end
            }

            # Serve content if verification passed
            try_files $uri $uri/ =404;
        }
    }
}
</code></pre>
<p>Create <code>/etc/nginx/lua/claudebot_verifier.lua</code>:</p>
<pre><code class="language-lua">local _M = {}

local function verify_dns(ip)
    -- Call external verification service or implement in Lua
    local handle = io.popen(&quot;python3 /usr/local/bin/verify_claudebot.py &quot; .. ip)
    local result = handle:read(&quot;*a&quot;)
    handle:close()
    return result:match(&quot;true&quot;) ~= nil
end

function _M.verify(ip)
    return verify_dns(ip)
end

return _M
</code></pre>
<p><strong>Apache configuration</strong> using mod_rewrite and external script:</p>
<pre><code class="language-apache">RewriteEngine On

# Check ClaudeBot User-Agent
RewriteCond %{HTTP_USER_AGENT} ClaudeBot [NC]

# Run verification script
RewriteCond %{REMOTE_ADDR}#%{HTTP_USER_AGENT} !^(.*)#(.*)$
RewriteRule .* - [E=VERIFIED:0]

# External verification
RewriteCond %{ENV:VERIFIED} =0
RewriteMap verify_bot prg:/usr/local/bin/verify-claudebot.sh
RewriteCond ${verify_bot:%{REMOTE_ADDR}} !=ok
RewriteRule .* - [F,L]
</code></pre>
<p><strong>Application middleware</strong> for dynamic platforms:</p>
<pre><code class="language-python"># Flask example
from flask import Flask, request, abort

app = Flask(__name__)

@app.before_request
def verify_crawler():
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    if &#39;claudebot&#39; in user_agent.lower():
        ip = request.remote_addr
        if not verify_claudebot(ip, user_agent):
            abort(403, description=&quot;ClaudeBot verification failed&quot;)
</code></pre>
<p><strong>CDN edge functions</strong> for Cloudflare Workers:</p>
<pre><code class="language-javascript">addEventListener(&#39;fetch&#39;, event =&gt; {
  event.respondWith(handleRequest(event.request))
})

async function handleRequest(request) {
  const userAgent = request.headers.get(&#39;User-Agent&#39;) || &#39;&#39;

  if (userAgent.toLowerCase().includes(&#39;claudebot&#39;)) {
    const ip = request.headers.get(&#39;CF-Connecting-IP&#39;)
    const verified = await verifyClaudeBot(ip)

    if (!verified) {
      return new Response(&#39;ClaudeBot verification failed&#39;, {
        status: 403
      })
    }
  }

  return fetch(request)
}

async function verifyClaudeBot(ip) {
  // Call DNS verification service
  const response = await fetch(`https://verify-api.example.com/check?ip=${ip}&amp;crawler=claudebot`)
  const data = await response.json()
  return data.verified
}
</code></pre>
<h2>Handling Verification Failures</h2>
<p>When verification fails, publishers must decide between blocking, logging, or challenging requests.</p>
<p><strong>Immediate blocking</strong> provides strongest protection:</p>
<pre><code class="language-python">if is_claudebot_claimed and not verify_claudebot(ip):
    return Response(&quot;Unauthorized crawler&quot;, status=403)
</code></pre>
<p><strong>Logging and monitoring</strong> tracks spoofing attempts:</p>
<pre><code class="language-python">if is_claudebot_claimed and not verify_claudebot(ip):
    logger.warning(f&quot;ClaudeBot spoofing attempt from {ip} with UA: {user_agent}&quot;)
    metrics.increment(&#39;claudebot.spoof_attempts&#39;)
    # Either block or allow with monitoring
</code></pre>
<p><strong>Progressive challenges</strong> escalate verification:</p>
<pre><code class="language-python">if is_claudebot_claimed and not verify_claudebot_ip(ip):
    # First failure: try DNS verification
    if not verify_claudebot_dns(ip):
        # Second failure: require CAPTCHA or block
        return challenge_response()
</code></pre>
<p><strong>Honeypot responses</strong> serve fake content to unverified crawlers:</p>
<pre><code class="language-python">if claimed_bot and not verified:
    return serve_honeypot_content()
</code></pre>
<p>Honeypot content includes unique identifiers enabling detection if spoofed crawler data appears in AI training corpuses.</p>
<h2>Frequently Asked Questions</h2>
<p><strong>How often should DNS verification be performed per IP?</strong></p>
<p>Cache verification results for 24-48 hours per IP. ClaudeBot infrastructure IPs don&#39;t change frequently enough to require per-request verification. Set cache TTLs balancing security (shorter = fresher verification) against performance (longer = fewer DNS queries). Monitor Anthropic announcements for infrastructure changes requiring cache invalidation.</p>
<p><strong>Can attackers bypass DNS verification by compromising Anthropic&#39;s DNS?</strong></p>
<p>Theoretically yes, but DNS compromise requires attacking Anthropic&#39;s registrar or nameservers—extremely difficult and quickly detected. DNS verification is far more secure than User-Agent checking alone. No practical verification method is perfectly secure; DNS bidirectional checking represents current best practice.</p>
<p><strong>What if ClaudeBot requests come from IPs not in published ranges or reverse DNS fails?</strong></p>
<p>Three scenarios: (1) Anthropic added new infrastructure not yet documented—temporary false negatives until ranges update, (2) Legitimate ClaudeBot using dynamic IPs or third-party infrastructure—contact Anthropic to clarify, (3) Spoofed crawler—correctly blocked. When uncertain, err toward blocking and monitor for patterns. Contact Anthropic if persistent verification failures suggest documentation gaps.</p>
<p><strong>Should verification happen on every request or only when robots.txt is violated?</strong></p>
<p>Verify on every ClaudeBot request for maximum security. Verification overhead (cached DNS lookups) adds milliseconds—negligible compared to content generation time. Only verifying robots.txt violations allows spoofed crawlers to access allowed content, potentially still problematic if publishers want complete control over who accesses what.</p>
<p><strong>How do I verify ClaudeBot when behind CDNs or load balancers?</strong></p>
<p>Extract real client IP from X-Forwarded-For or CF-Connecting-IP headers rather than immediate connection IP (which would be CDN/load balancer). Configure trust for these headers carefully—only accept from known infrastructure IPs to prevent header spoofing. Most CDNs provide ways to validate that forwarded IP headers are authentic.</p>
<p><strong>Does ClaudeBot provide any official verification APIs or services?</strong></p>
<p>As of early 2026, Anthropic hasn&#39;t announced official verification APIs. Publishers perform verification through DNS lookups and published IP ranges. Future official APIs could simplify verification—check Anthropic documentation for updates. Some crawler operators (Google, Bing) provide verification endpoints; similar services from Anthropic would improve ecosystem reliability.</p>

      </article>

      <div class="cta-box" style="margin: var(--sp-16) 0;">
        <h3>Your Content Feeds AI. Get Paid for It.</h3>
        <p>Complete pay-per-crawl implementation. Templates, pricing models, licensing contracts. Everything.</p>
        <a href="/setup.html" class="btn btn--primary btn--large">Master the Protocol &mdash; $2,497</a>
      </div>

      <div style="margin-top: var(--sp-8); padding-top: var(--sp-8); border-top: 1px solid var(--border);">
        <a href="/articles.html" style="font-family: var(--font-mono); font-size: 0.875rem; font-weight: 500; color: var(--accent);">&larr; All Articles</a>
      </div>
    </div>
  </main>


  <footer class="footer">
    <div class="container">
      <div class="footer__grid">
        <div class="footer__brand">
          <div class="footer__logo">AI Pay Per Crawl</div>
          <p class="footer__desc">Your content feeds AI. The protocol ensures you get paid for it. Implementation guides, pricing models, and licensing infrastructure for publishers monetizing AI crawler traffic.</p>
          <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
        </div>

        <div>
          <div class="footer__heading">Implementation</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Cloudflare Setup</a></li>
            <li><a href="/articles.html">RSL Protocol</a></li>
            <li><a href="/articles.html">llms.txt Guide</a></li>
            <li><a href="/articles.html">Nginx Rules</a></li>
            <li><a href="/articles.html">Apache Config</a></li>
            <li><a href="/articles.html">WordPress Plugin</a></li>
            <li><a href="/articles.html">CDN Integration</a></li>
            <li><a href="/articles.html">robots.txt Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">Pricing &amp; Tools</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Pricing Models</a></li>
            <li><a href="/articles.html">Rate Cards</a></li>
            <li><a href="/articles.html">Revenue Calculator</a></li>
            <li><a href="/articles.html">Crawler Directory</a></li>
            <li><a href="/articles.html">Bot Detection</a></li>
            <li><a href="/articles.html">Compliance Rates</a></li>
            <li><a href="/articles.html">Deal Analysis</a></li>
            <li><a href="/articles.html">Licensing Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">From Scale With Search</div>
          <ul class="footer__links">
            <li><a href="https://scalewithsearch.com" target="_blank" rel="me">Scale With Search</a></li>
            <li><a href="https://aifirstsearch.com" target="_blank" rel="me">AI First Search</a></li>
            <li><a href="https://browserprompt.com" target="_blank" rel="me">Browser Prompt</a></li>
            <li><a href="https://polytraffic.com" target="_blank" rel="me">Polytraffic</a></li>
            <li><a href="https://creatinepedia.com" target="_blank" rel="me">Creatinepedia</a></li>
            <li><a href="https://victorvalentineromo.com" target="_blank" rel="me">Victor Romo</a></li>
            <li><a href="https://b2bvic.com" target="_blank" rel="me">B2B Vic</a></li>
          </ul>
        </div>
      </div>

      <div class="footer__bar">
        <p>&copy; 2026 AI Pay Per Crawl. Built by <a href="https://victorvalentineromo.com" target="_blank">Victor Valentine Romo</a>.</p>
        <p>A <a href="https://scalewithsearch.com" target="_blank">Scale With Search</a> property.</p>
      </div>
    </div>
  </footer>


  <script>
    /* Mega menu */
    (function() {
      var triggers = document.querySelectorAll('.mega-trigger');
      var panels = document.querySelectorAll('.mega-panel');
      var closeTimeout = null;

      function openPanel(id) {
        clearTimeout(closeTimeout);
        panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
        triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        var panel = document.getElementById('mega-' + id);
        var trigger = document.querySelector('[data-mega="' + id + '"] .mega-trigger');
        if (panel) panel.classList.add('mega-panel--open');
        if (trigger) trigger.setAttribute('aria-expanded', 'true');
      }

      function scheduleClose() {
        closeTimeout = setTimeout(function() {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }, 200);
      }

      function cancelClose() { clearTimeout(closeTimeout); }

      triggers.forEach(function(btn) {
        var wrapper = btn.closest('[data-mega]');
        var id = wrapper.getAttribute('data-mega');
        btn.addEventListener('mouseenter', function() { openPanel(id); });
        btn.addEventListener('click', function(e) {
          e.preventDefault();
          var panel = document.getElementById('mega-' + id);
          if (panel && panel.classList.contains('mega-panel--open')) {
            panel.classList.remove('mega-panel--open');
            btn.setAttribute('aria-expanded', 'false');
          } else {
            openPanel(id);
          }
        });
        wrapper.addEventListener('mouseleave', scheduleClose);
      });

      panels.forEach(function(panel) {
        panel.addEventListener('mouseenter', cancelClose);
        panel.addEventListener('mouseleave', scheduleClose);
      });

      document.addEventListener('click', function(e) {
        if (!e.target.closest('[data-mega]') && !e.target.closest('.mega-panel')) {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });

      document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape') {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });
    })();

    /* Mobile menu */
    var mobileMenuBtn = document.getElementById('mobile-menu-btn');
    var mobileMenu = document.getElementById('mobile-menu');
    var menuIcon = document.getElementById('menu-icon');
    var closeIcon = document.getElementById('close-icon');
    if (mobileMenuBtn) {
      mobileMenuBtn.addEventListener('click', function() {
        mobileMenu.classList.toggle('mobile-menu--open');
        menuIcon.classList.toggle('hidden');
        closeIcon.classList.toggle('hidden');
      });
    }

    /* Self-link neutralizer */
    (function() {
      var currentPath = window.location.pathname.replace(/\.html$/, '').replace(/\/$/, '') || '/';
      document.querySelectorAll('a[href]').forEach(function(a) {
        var href = a.getAttribute('href').replace(/\.html$/, '').replace(/\/$/, '') || '/';
        if (href === currentPath) {
          a.removeAttribute('href');
          a.setAttribute('aria-current', 'page');
          a.style.opacity = '0.5';
          a.style.pointerEvents = 'none';
        }
      });
    })();

    /* Copy code button */
    function copyCode(button) {
      var codeBlock = button.closest('.code-block');
      var code = codeBlock.querySelector('code').innerText;
      navigator.clipboard.writeText(code).then(function() {
        button.textContent = 'Copied!';
        setTimeout(function() { button.textContent = 'Copy'; }, 2000);
      });
    }

    /* FAQ Accordion */
    document.querySelectorAll('.accordion__trigger').forEach(function(trigger) {
      trigger.addEventListener('click', function() {
        var expanded = this.getAttribute('aria-expanded') === 'true';
        var body = this.nextElementSibling;
        this.setAttribute('aria-expanded', !expanded);
        body.classList.toggle('accordion__body--open');
      });
    });
  </script>
</body>
</html>