<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Crawler Paywall Strategies: Gating Content for Bot Access | AI Pay Per Crawl</title>
    <meta name="description" content="Technical paywall strategies for monetizing AI crawler traffic. Implementation methods for differential content access, user-agent gating, and pay-to-crawl infrastructure.">
    <meta name="author" content="Victor Valentine Romo">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="AI Crawler Paywall Strategies: Gating Content for Bot Access">
    <meta property="og:description" content="Technical paywall strategies for monetizing AI crawler traffic. Implementation methods for differential content access, user-agent gating, and pay-to-crawl infrastructure.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://aipaypercrawl.com/articles/ai-crawler-paywall-strategies">
    <meta property="og:site_name" content="AI Pay Per Crawl">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="AI Crawler Paywall Strategies: Gating Content for Bot Access">
    <meta name="twitter:description" content="Technical paywall strategies for monetizing AI crawler traffic. Implementation methods for differential content access, user-agent gating, and pay-to-crawl infrastructure.">
    <link rel="canonical" href="https://aipaypercrawl.com/articles/ai-crawler-paywall-strategies">
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "TechArticle",
  "headline": "AI Crawler Paywall Strategies: Gating Content for Bot Access",
  "description": "Technical paywall strategies for monetizing AI crawler traffic. Implementation methods for differential content access, user-agent gating, and pay-to-crawl infrastructure.",
  "author": {
    "@type": "Person",
    "@id": "https://victorvalentineromo.com/#person",
    "name": "Victor Valentine Romo",
    "url": "https://victorvalentineromo.com"
  },
  "publisher": {
    "@type": "Organization",
    "@id": "https://aipaypercrawl.com/#organization",
    "name": "AI Pay Per Crawl",
    "url": "https://aipaypercrawl.com"
  },
  "datePublished": "2026-02-08",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aipaypercrawl.com/articles/ai-crawler-paywall-strategies"
  }
}
    </script>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://aipaypercrawl.com"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Articles",
      "item": "https://aipaypercrawl.com/articles.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "AI Crawler Paywall Strategies: Gating Content for Bot Access",
      "item": "https://aipaypercrawl.com/articles/ai-crawler-paywall-strategies"
    }
  ]
}
    </script>

    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect fill='%234f46e5' width='100' height='100' rx='12'/><text x='50' y='70' font-family='monospace' font-size='38' font-weight='700' fill='%23ffffff' text-anchor='middle'>APC</text></svg>">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet"></noscript>
    <link rel="stylesheet" href="/base.css">
    <link rel="me" href="https://scalewithsearch.com">
    <link rel="me" href="https://victorvalentineromo.com">
    <link rel="me" href="https://aifirstsearch.com">
    <link rel="me" href="https://browserprompt.com">
    <link rel="me" href="https://creatinepedia.com">
    <link rel="me" href="https://polytraffic.com">
    <link rel="me" href="https://tattooremovalnear.com">
    <link rel="me" href="https://comicstripai.com">
    <link rel="me" href="https://aipaypercrawl.com">
    <link rel="me" href="https://b2bvic.com">
    <link rel="me" href="https://seobyrole.com">
    <link rel="me" href="https://quickfixseo.com">
</head>
<body>


  <nav class="nav" role="navigation" aria-label="Primary">
    <div class="nav__inner">
      <a href="/" class="nav__logo">APC</a>

      <div class="nav__links">
        <!-- Implementation dropdown -->
        <div class="mega-wrapper" data-mega="implementation">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-implementation">
            Implementation
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Pricing dropdown -->
        <div class="mega-wrapper" data-mega="pricing">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-pricing">
            Pricing
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Crawlers dropdown -->
        <div class="mega-wrapper" data-mega="crawlers">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-crawlers">
            Crawlers
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Legal (direct link) -->
        <a href="/articles.html" class="nav__link">Legal</a>

        <div class="nav__divider"></div>

        <a href="/setup.html" class="nav__cta">Master the Protocol &mdash; $2,497</a>
      </div>

      <button class="nav__mobile-btn" id="mobile-menu-btn" aria-label="Open menu">
        <svg id="menu-icon" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
        <svg id="close-icon" class="hidden" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
      </button>
    </div>

    <!-- Mega Panel: Implementation -->
    <div id="mega-implementation" class="mega-panel" role="region" aria-label="Implementation guides">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Infrastructure</div>
          <a href="/articles.html" class="mega-link">Cloudflare Setup</a>
          <a href="/articles.html" class="mega-link">Nginx Rules</a>
          <a href="/articles.html" class="mega-link">Apache Config</a>
          <a href="/articles.html" class="mega-link">WordPress Plugin</a>
          <a href="/articles.html" class="mega-link">CDN Integration</a>
        </div>
        <div>
          <div class="mega-column__label">Protocols</div>
          <a href="/articles.html" class="mega-link">RSL Protocol</a>
          <a href="/articles.html" class="mega-link">llms.txt Specification</a>
          <a href="/articles.html" class="mega-link">robots.txt for AI</a>
          <a href="/articles.html" class="mega-link">Machine-Readable Terms</a>
        </div>
        <div>
          <div class="mega-column__label">Quick Start</div>
          <div style="background: linear-gradient(135deg, rgba(16,185,129,0.06), rgba(79,70,229,0.04)); border: 1px solid rgba(16,185,129,0.2); border-radius: 12px; padding: 1.5rem;">
            <h3 style="font-family: var(--font-display); font-size: 1.125rem; font-weight: 600; color: var(--text-primary); margin-bottom: 0.5rem;">Skip the learning curve.</h3>
            <p style="font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 1rem;">Complete pay-per-crawl implementation. Templates, pricing, contracts.</p>
            <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
          </div>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Pricing -->
    <div id="mega-pricing" class="mega-panel" role="region" aria-label="Pricing models">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Models</div>
          <a href="/articles.html" class="mega-link">Per-Crawl Pricing</a>
          <a href="/articles.html" class="mega-link">Flat-Rate Annual</a>
          <a href="/articles.html" class="mega-link">Tiered Pricing</a>
          <a href="/articles.html" class="mega-link">Volume Discounts</a>
          <a href="/articles.html" class="mega-link">Hybrid Models</a>
        </div>
        <div>
          <div class="mega-column__label">Benchmarks</div>
          <a href="/articles.html" class="mega-link">Rate Cards by Content Type</a>
          <a href="/articles.html" class="mega-link">Revenue Calculators</a>
          <a href="/articles.html" class="mega-link">Industry Benchmarks</a>
          <a href="/articles.html" class="mega-link">Deal Comparisons</a>
        </div>
        <div>
          <div class="mega-column__label">Recent Deals</div>
          <a href="/articles.html" class="mega-link">News Corp &times; OpenAI ($250M)</a>
          <a href="/articles.html" class="mega-link">Reddit &times; Google ($60M/yr)</a>
          <a href="/articles.html" class="mega-link">FT &times; Anthropic</a>
          <a href="/articles.html" class="mega-link">AP &times; OpenAI</a>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Crawlers -->
    <div id="mega-crawlers" class="mega-panel" role="region" aria-label="Crawler information">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Major Crawlers</div>
          <a href="/articles.html" class="mega-link">GPTBot (OpenAI)</a>
          <a href="/articles.html" class="mega-link">ClaudeBot (Anthropic)</a>
          <a href="/articles.html" class="mega-link">Googlebot-Extended</a>
          <a href="/articles.html" class="mega-link">Bytespider (ByteDance)</a>
          <a href="/articles.html" class="mega-link">CCBot (Common Crawl)</a>
        </div>
        <div>
          <div class="mega-column__label">Detection</div>
          <a href="/articles.html" class="mega-link">Bot Detection Methods</a>
          <a href="/articles.html" class="mega-link">User Agent Reference</a>
          <a href="/articles.html" class="mega-link">IP Range Verification</a>
          <a href="/articles.html" class="mega-link">Compliance Rates</a>
        </div>
        <div>
          <div class="mega-column__label">Directory</div>
          <a href="/articles.html" class="mega-link">Full Crawler Directory</a>
          <a href="/articles.html" class="mega-link">Crawler Behavior Matrix</a>
          <a href="/articles.html" class="mega-link">robots.txt Respect Rates</a>
        </div>
      </div>
    </div>

    <!-- Mobile Menu -->
    <div id="mobile-menu" class="mobile-menu">
      <a href="/articles.html" class="mobile-menu__link">Implementation</a>
      <a href="/articles.html" class="mobile-menu__link">Pricing</a>
      <a href="/articles.html" class="mobile-menu__link">Crawlers</a>
      <a href="/articles.html" class="mobile-menu__link">Legal</a>
      <a href="/articles.html" class="mobile-menu__link">All Articles</a>
      <a href="/setup.html" class="mobile-menu__cta">Master the Protocol &mdash; $2,497</a>
    </div>
  </nav>

  <main class="pt-nav">
    <div class="container--narrow" style="padding-top: var(--sp-8);">
      <div class="breadcrumbs">
        <a href="/">Home</a><span class="breadcrumbs__sep">/</span>
        <a href="/articles.html">Articles</a><span class="breadcrumbs__sep">/</span>
        <span>AI Crawler Paywall Strategies: Gating Content for Bot Access</span>
      </div>

      <header style="margin-bottom: var(--sp-12);">
        <span class="label" style="margin-bottom: var(--sp-4); display: block;">Article &middot; 25 min read</span>
        <h1>AI Crawler Paywall Strategies: Gating Content for Bot Access</h1>
        <p style="font-size: 1.125rem; color: var(--text-secondary); margin-top: var(--sp-4); max-width: 640px;">Technical paywall strategies for monetizing AI crawler traffic. Implementation methods for differential content access, user-agent gating, and pay-to-crawl infrastructure.</p>
      </header>

      <article class="article-body">
        <h1>AI Crawler Paywall Strategies: Gating Content for Bot Access</h1>
<p>Your content sits behind a paywall. Subscribers pay $10/month. Revenue model calibrated to human readers. <strong>But AI crawlers don&#39;t subscribe.</strong></p>
<p><strong>GPTBot</strong> scrapes paywalled content. <strong>ClaudeBot</strong> indexes research you sell to members. <strong>Bytespider</strong> copies proprietary analysis. Default behavior: Crawlers penetrate paywalls designed for humans. Your gating mechanisms fail against bots.</p>
<p>The problem compounds. Subscribers discover AI systems synthesize paywalled insights freely. Value proposition erodes. Why pay for content when <strong>ChatGPT</strong> provides summaries trained on material you monetize?</p>
<p><strong>Publishers are building bot-specific paywalls.</strong> Not human gates (session cookies, account logins) but crawler-targeted barriers. Differential access architectures serving distinct content to human subscribers versus AI systems requesting training data.</p>
<p><strong>Three paywall strategies emerged:</strong></p>
<ol>
<li><strong>Selective gating</strong> (allow search crawlers, block training bots)</li>
<li><strong>Freemium content stratification</strong> (free tier for bots, premium requires licensing)</li>
<li><strong>Pay-to-crawl infrastructure</strong> (technical gating requiring payment authentication)</li>
</ol>
<p>Each approach solves distinct business objectives. News organizations prioritizing discovery use selective gating. Publishers monetizing via licensing deploy freemium stratification. Platforms with technical capability implement pay-to-crawl systems extracting direct revenue.</p>
<p>This guide details implementation methods for each strategy. Technical architectures, enforcement mechanisms, revenue optimization, and hybrid deployment combining multiple approaches.</p>
<h2>Understanding Bot-Specific Paywalls</h2>
<h3>Why Human Paywalls Fail Against Bots</h3>
<p><strong>Human paywall architecture:</strong></p>
<p>Session-based authentication. User logs in, receives session cookie. Subsequent requests validate cookie. Content served if authenticated.</p>
<p><strong>Bot bypass vectors:</strong></p>
<p><strong>1. No cookie persistence requirement</strong></p>
<p>Crawlers issue independent GET requests. No session continuity. If content leaks via direct URL access (bypassing login flow), bots capture it.</p>
<p><strong>Example vulnerability:</strong></p>
<pre><code># Paywall protects homepage
https://site.com/ → requires login

# Direct article access leaks content
https://site.com/articles/premium-research.html → no auth check
</code></pre>
<p><strong>Bot requests article directly.</strong> Server validates session cookie. <strong>No cookie present</strong>—but misconfigured logic serves content anyway (assumes referrer from authenticated page).</p>
<p><strong>2. JavaScript execution avoidance</strong></p>
<p>Many paywalls use client-side enforcement. JavaScript checks authentication status, hides content if unauthenticated.</p>
<p><strong>HTML source contains full article.</strong> JavaScript overlays paywall modal. Bots read HTML, ignore JavaScript, extract complete text.</p>
<p><strong>3. API endpoint exposure</strong></p>
<p>Modern sites use APIs. Frontend requests:</p>
<pre><code>GET /api/articles/12345
Authorization: Bearer [token]
</code></pre>
<p><strong>If API lacks token validation:</strong> Bot requests API directly, receives JSON response with full content.</p>
<p><strong>4. Search engine exemptions</strong></p>
<p>Google requires &quot;First Click Free&quot; (now &quot;Flexible Sampling&quot;). Publishers show full article to Googlebot to maintain search indexing.</p>
<p><strong>Implementation:</strong></p>
<pre><code class="language-python">if user_agent == &quot;Googlebot&quot;:
    serve_full_content()
else:
    serve_paywalled_version()
</code></pre>
<p><strong>Unintended consequence:</strong> AI crawlers spoof Googlebot user agent, receive full content.</p>
<p><strong>Human paywalls optimize for user friction reduction</strong> (minimize login barriers, preserve reader experience). <strong>Bot paywalls optimize for access control</strong> (aggressive verification, zero tolerance for authentication bypass).</p>
<h3>Distinguishing Search Crawlers From Training Bots</h3>
<p><strong>Critical distinction:</strong> Not all bots are equivalent.</p>
<p><strong>Search crawlers</strong> (Googlebot, Bingbot) drive traffic. Blocking damages discovery. <strong>Training bots</strong> (GPTBot, ClaudeBot) extract content for model training. Blocking protects IP but generates zero traffic value.</p>
<p><strong>Selective gating strategy:</strong> Allow search indexing, block AI training.</p>
<p><strong>User agent identification:</strong></p>
<pre><code># Search engines
Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)
Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)

# AI training bots
Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)
Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; ClaudeBot/1.0; +https://www.anthropic.com)
</code></pre>
<p><strong>robots.txt selective blocking:</strong></p>
<pre><code># Allow search engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

# Block AI training
User-agent: GPTBot
Disallow: /

User-agent: ClaudeBot
Disallow: /

User-agent: Bytespider
Disallow: /
</code></pre>
<p><strong>Effect:</strong> Google indexes your content (drives SEO traffic). OpenAI blocked from scraping (must license or pay for access).</p>
<p><strong>Verification requirement:</strong> Robots.txt is advisory. Enforcement requires IP verification and server-level blocking. Full implementation: <a href="block-all-ai-crawlers-robots-txt.html">block-all-ai-crawlers-robots-txt.html</a></p>
<h3>Legal and Compliance Considerations</h3>
<p><strong>Paywall gating for bots = legal gray area.</strong></p>
<p><strong>Publisher rights:</strong></p>
<ol>
<li><strong>Copyright ownership</strong> (content belongs to publisher)</li>
<li><strong>Terms of Service enforcement</strong> (site access subject to conditions)</li>
<li><strong>Trespass to chattels</strong> (unauthorized server access consumes resources)</li>
</ol>
<p><strong>AI company arguments:</strong></p>
<ol>
<li><strong>Fair use</strong> (transformative training use)</li>
<li><strong>Publicly accessible content</strong> (no authentication required)</li>
<li><strong>Robots.txt compliance</strong> (voluntary standard, not law)</li>
</ol>
<p><strong>Courts haven&#39;t definitively ruled on AI training copyright status.</strong> Multiple lawsuits pending (NYT v OpenAI, Getty v Stability AI, Authors Guild class action).</p>
<p><strong>Prudent publisher strategy:</strong></p>
<p><strong>1. Explicit Terms of Service</strong></p>
<pre><code>CONTENT LICENSE RESTRICTIONS

Automated access to Content for purposes of training artificial
intelligence systems, large language models, or machine learning
algorithms is prohibited without express written permission.

Violation of these Terms grants Publisher the right to:
- Block access (IP-level bans, technical countermeasures)
- Seek injunctive relief (court orders stopping scraping)
- Pursue statutory damages (copyright infringement claims)
</code></pre>
<p><strong>2. Copyright registration</strong></p>
<p>Register high-value content with U.S. Copyright Office. Prerequisite for statutory damages claims (up to $150K per work infringed).</p>
<p><strong>3. DMCA takedown readiness</strong></p>
<p>If AI system reproduces copyrighted content verbatim, issue DMCA takedown to training data repositories (Common Crawl, C4 dataset).</p>
<p><strong>4. Technical enforcement</strong></p>
<p>Don&#39;t rely solely on legal threats. Implement technical blocks making scraping expensive (rate limiting, IP blocking, content obfuscation).</p>
<p><strong>Compliance obligation:</strong> Ensure bot blocking doesn&#39;t violate accessibility laws (ADA, WCAG). Human users with disabilities must retain full access. Bot gates target automated systems, not assistive technologies.</p>
<h2>Strategy 1: Selective Gating Architecture</h2>
<h3>Allowing Search While Blocking Training</h3>
<p><strong>Objective:</strong> Maintain SEO benefits while protecting content from AI training.</p>
<p><strong>Technical implementation:</strong></p>
<p><strong>User agent detection middleware:</strong></p>
<pre><code class="language-python"># Define allowed vs. blocked crawlers
SEARCH_CRAWLERS = [&#39;Googlebot&#39;, &#39;Bingbot&#39;, &#39;DuckDuckBot&#39;]
TRAINING_BOTS = [&#39;GPTBot&#39;, &#39;ClaudeBot&#39;, &#39;Bytespider&#39;, &#39;CCBot&#39;]

@app.before_request
def check_bot_access():
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    # Check if training bot
    for bot in TRAINING_BOTS:
        if bot.lower() in user_agent.lower():
            # Block with 403 Forbidden
            return render_template(&#39;bot_licensing_required.html&#39;), 403

    # Check if search crawler
    for crawler in SEARCH_CRAWLERS:
        if crawler.lower() in user_agent.lower():
            # Verify IP (prevent spoofing)
            if verify_search_engine_ip(request.remote_addr, crawler):
                request.crawler_type = &#39;search&#39;
                return None  # Allow access

    # Human or unknown bot - standard paywall logic
    request.crawler_type = &#39;unknown&#39;
    return None
</code></pre>
<p><strong>IP verification prevents spoofing:</strong></p>
<pre><code class="language-python">import socket
import dns.resolver

def verify_search_engine_ip(ip_address, crawler_name):
    &quot;&quot;&quot;Verify IP belongs to declared search engine via reverse DNS&quot;&quot;&quot;

    try:
        # Reverse DNS lookup
        hostname = socket.gethostbyaddr(ip_address)[0]

        # Verify hostname matches expected domain
        if crawler_name == &#39;Googlebot&#39;:
            if not hostname.endswith((&#39;.googlebot.com&#39;, &#39;.google.com&#39;)):
                return False
        elif crawler_name == &#39;Bingbot&#39;:
            if not hostname.endswith(&#39;.search.msn.com&#39;):
                return False

        # Forward DNS lookup (confirm hostname resolves to original IP)
        resolved_ip = socket.gethostbyname(hostname)
        return resolved_ip == ip_address

    except (socket.herror, socket.gaierror):
        return False  # DNS lookup failed
</code></pre>
<p><strong>Why verification matters:</strong> Without IP checks, AI bots spoof &quot;Googlebot&quot; user agent, bypass paywall. Reverse DNS confirms authenticity.</p>
<p><strong>nginx implementation:</strong></p>
<pre><code class="language-nginx"># Map user agents to access levels
map $http_user_agent $bot_access {
    default &quot;unknown&quot;;
    ~*Googlebot &quot;search&quot;;
    ~*Bingbot &quot;search&quot;;
    ~*GPTBot &quot;training_blocked&quot;;
    ~*ClaudeBot &quot;training_blocked&quot;;
    ~*Bytespider &quot;training_blocked&quot;;
}

# Block training bots
location / {
    if ($bot_access = &quot;training_blocked&quot;) {
        return 403 &quot;Content licensing required. Contact licensing@site.com&quot;;
    }

    # Continue to paywall logic for humans
    proxy_pass http://backend;
}
</code></pre>
<p><strong>Cloudflare WAF rules:</strong></p>
<p>Custom firewall rule blocking training bots:</p>
<pre><code>(http.user_agent contains &quot;GPTBot&quot; or
 http.user_agent contains &quot;ClaudeBot&quot; or
 http.user_agent contains &quot;Bytespider&quot;)
→ Block
</code></pre>
<p><strong>Result:</strong> Search engines crawl freely (SEO preserved). Training bots receive 403 error page with licensing contact information.</p>
<h3>Content Sampling Techniques</h3>
<p><strong>Full blocking may not be optimal.</strong> Alternative: Provide samples demonstrating content value, require licensing for complete access.</p>
<p><strong>Strategy: Snippet sampling</strong></p>
<p>Serve first 300 words to training bots. Truncate remainder.</p>
<p><strong>Implementation:</strong></p>
<pre><code class="language-python">@app.route(&#39;/articles/&lt;article_id&gt;&#39;)
def serve_article(article_id):
    article = fetch_article(article_id)
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    # Full content for subscribers
    if is_authenticated_subscriber(request):
        return render_template(&#39;article_full.html&#39;, article=article)

    # Sample for training bots
    if is_training_bot(user_agent):
        article.content = truncate_content(article.content, max_words=300)
        article.truncated = True
        return render_template(&#39;article_sample.html&#39;, article=article)

    # Standard paywall for humans
    return render_template(&#39;article_paywall.html&#39;, article=article)
</code></pre>
<p><strong>Benefits:</strong></p>
<ol>
<li><strong>Demonstrates content value</strong> (AI company sees quality, motivated to license)</li>
<li><strong>Reduces crawl bandwidth</strong> (smaller payloads)</li>
<li><strong>Maintains discoverability</strong> (bots index topics, not full text)</li>
</ol>
<p><strong>robots.txt meta tag alternative:</strong></p>
<pre><code class="language-html">&lt;meta name=&quot;robots&quot; content=&quot;max-snippet:300&quot;&gt;
</code></pre>
<p><strong>Effect:</strong> Instructs compliant crawlers to limit indexed text to 300 characters. <strong>Limitation:</strong> Not all bots honor meta directives. Server-side truncation more reliable.</p>
<p><strong>Graduated sampling tiers:</strong></p>
<pre><code class="language-python">SAMPLING_TIERS = {
    &#39;Googlebot&#39;: &#39;full&#39;,      # Full content (SEO priority)
    &#39;GPTBot&#39;: &#39;sample_300&#39;,   # 300-word sample
    &#39;ClaudeBot&#39;: &#39;sample_300&#39;,
    &#39;Unknown&#39;: &#39;block&#39;        # Unknown bots blocked entirely
}
</code></pre>
<p><strong>Adjust sampling based on negotiation progress.</strong> If AI company enters licensing discussion, increase sample size (500 words) as goodwill gesture.</p>
<h3>Dynamic Enforcement Based on Usage</h3>
<p><strong>Adaptive gating:</strong> Allow limited scraping, block if volume exceeds threshold.</p>
<p><strong>Use case:</strong> AI company scrapes 10K pages. Acceptable (minimal bandwidth). Scrapes 500K pages. Unacceptable (must license).</p>
<p><strong>Rate-based enforcement:</strong></p>
<pre><code class="language-python">from redis import Redis
from datetime import datetime, timedelta

redis = Redis()

@app.before_request
def rate_limit_bots():
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    if not is_training_bot(user_agent):
        return None  # Not a training bot

    bot_id = identify_bot(user_agent)
    ip = request.remote_addr
    key = f&quot;bot_requests:{bot_id}:{ip}:{datetime.now().strftime(&#39;%Y-%m&#39;)}&quot;

    # Increment monthly request counter
    current_count = redis.incr(key)
    redis.expire(key, int(timedelta(days=32).total_seconds()))

    # Check threshold
    FREE_TIER_LIMIT = 10000  # 10K requests/month free

    if current_count &gt; FREE_TIER_LIMIT:
        return render_template(&#39;licensing_required.html&#39;,
                              current_usage=current_count,
                              limit=FREE_TIER_LIMIT), 403

    # Under limit - allow access
    return None
</code></pre>
<p><strong>Graduated enforcement:</strong></p>
<table>
<thead>
<tr>
<th>Requests/Month</th>
<th>Action</th>
</tr>
</thead>
<tbody><tr>
<td>0-10K</td>
<td>Allow (free tier)</td>
</tr>
<tr>
<td>10K-50K</td>
<td>Throttle (rate limit to 1 req/sec)</td>
</tr>
<tr>
<td>50K+</td>
<td>Block with licensing prompt</td>
</tr>
</tbody></table>
<p><strong>Benefits:</strong></p>
<ol>
<li><strong>Low-friction entry</strong> (small-scale experimentation allowed)</li>
<li><strong>Automatic monetization trigger</strong> (heavy use → licensing conversation)</li>
<li><strong>Proportional enforcement</strong> (light scraping tolerated, extraction blocked)</li>
</ol>
<p><strong>Notification system:</strong> When bot crosses threshold, email: &quot;GPTBot has accessed 50,000 pages this month. Our licensing tier for this volume is $X. Contact us to continue access.&quot;</p>
<h2>Strategy 2: Freemium Content Stratification</h2>
<h3>Structuring Free vs. Premium Tiers</h3>
<p><strong>Not all content has equal value.</strong> Strategic differentiation enables dual objectives: discoverability (free tier) + monetization (premium tier).</p>
<p><strong>Free tier content:</strong></p>
<ul>
<li>News summaries (brief, 200-400 word articles)</li>
<li>General analysis (broad topic overviews)</li>
<li>Older archives (content &gt;2 years old)</li>
<li>Public domain material (government data, press releases)</li>
</ul>
<p><strong>Premium tier content:</strong></p>
<ul>
<li>Investigative journalism (in-depth reports, 2,000+ words)</li>
<li>Proprietary research (original data collection)</li>
<li>Expert interviews (exclusive access)</li>
<li>Real-time coverage (breaking news, live updates)</li>
<li>Subscriber-only newsletters</li>
</ul>
<p><strong>Rationale:</strong> AI systems benefit from free tier (general knowledge, context). Premium tier represents differentiated value justifying licensing fees.</p>
<p><strong>Implementation via URL structure:</strong></p>
<pre><code>/news/           → Free tier (all bots allowed)
/archive/        → Free tier (bots allowed)
/premium/        → Premium tier (licensing required)
/research/       → Premium tier (licensing required)
/subscribers/    → Premium tier (licensing required)
</code></pre>
<p><strong>robots.txt configuration:</strong></p>
<pre><code>User-agent: GPTBot
Allow: /news/
Allow: /archive/
Disallow: /premium/
Disallow: /research/
Disallow: /subscribers/
</code></pre>
<p><strong>Effect:</strong> GPTBot indexes free content (builds awareness of publisher brand and general topics). Premium content blocked unless licensed.</p>
<h3>Licensing Contact Points in Gated Content</h3>
<p><strong>When training bot hits premium paywall, convert friction into licensing opportunity.</strong></p>
<p><strong>403 error page (bot-specific):</strong></p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Content Licensing Required&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;AI Content Licensing&lt;/h1&gt;

    &lt;p&gt;This premium content requires a licensing agreement for AI training access.&lt;/p&gt;

    &lt;h2&gt;Our Content Library Includes:&lt;/h2&gt;
    &lt;ul&gt;
        &lt;li&gt;50,000+ investigative articles (2018-present)&lt;/li&gt;
        &lt;li&gt;Proprietary industry research and data&lt;/li&gt;
        &lt;li&gt;Expert interviews and exclusive analysis&lt;/li&gt;
        &lt;li&gt;Real-time coverage and breaking news&lt;/li&gt;
    &lt;/ul&gt;

    &lt;h2&gt;Licensing Options:&lt;/h2&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;strong&gt;Annual License:&lt;/strong&gt; $250,000/year (unlimited access)&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Usage-Based:&lt;/strong&gt; $0.01 per article accessed&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;API Access:&lt;/strong&gt; Custom pricing for structured data feeds&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;strong&gt;Contact:&lt;/strong&gt; licensing@yoursite.com&lt;/p&gt;
    &lt;p&gt;&lt;strong&gt;Technical Documentation:&lt;/strong&gt; &lt;a href=&quot;https://yoursite.com/licensing-api&quot;&gt;API Specs&lt;/a&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong>Key elements:</strong></p>
<ol>
<li><strong>Value proposition</strong> (quantify content library)</li>
<li><strong>Pricing transparency</strong> (show licensing costs upfront)</li>
<li><strong>Multiple options</strong> (flat-fee, usage-based, API)</li>
<li><strong>Clear CTA</strong> (contact email, documentation link)</li>
</ol>
<p><strong>Conversion tracking:</strong> Log which bots hit licensing pages. Indicates interest level.</p>
<pre><code class="language-python">@app.errorhandler(403)
def handle_bot_block(error):
    if is_training_bot(request.headers.get(&#39;User-Agent&#39;)):
        # Log potential customer
        log_bot_licensing_interest(
            bot=identify_bot(request.headers.get(&#39;User-Agent&#39;)),
            url=request.url,
            timestamp=datetime.now()
        )
        return render_template(&#39;bot_licensing_required.html&#39;), 403
    else:
        return render_template(&#39;generic_403.html&#39;), 403
</code></pre>
<p><strong>Sales follow-up:</strong> Monthly review logs. If GPTBot hits licensing page 500+ times, proactively contact OpenAI: &quot;We&#39;ve observed significant interest in our content. Let&#39;s discuss licensing terms.&quot;</p>
<h3>Revenue Optimization Across Tiers</h3>
<p><strong>Maximize total revenue = free tier value (brand awareness, SEO) + premium tier revenue (licensing fees).</strong></p>
<p><strong>Free tier revenue drivers:</strong></p>
<ol>
<li><strong>Search traffic</strong> (free content indexed → drives organic visits → ad revenue)</li>
<li><strong>Brand positioning</strong> (AI systems reference your content → credibility boost)</li>
<li><strong>Conversion funnel</strong> (free tier introduces brand, premium tier monetizes)</li>
</ol>
<p><strong>Premium tier revenue:</strong></p>
<p>Direct licensing fees from AI companies needing high-value content.</p>
<p><strong>Optimization framework:</strong></p>
<p><strong>1. Content classification</strong></p>
<p>Audit existing content. Tag each article:</p>
<ul>
<li><strong>Commodity:</strong> Widely available elsewhere (free tier)</li>
<li><strong>Differentiated:</strong> Unique perspective but not exclusive (free tier)</li>
<li><strong>Proprietary:</strong> Original research, exclusive access (premium tier)</li>
</ul>
<p><strong>Migration rule:</strong> Move all proprietary content behind premium paywall.</p>
<p><strong>2. Value quantification</strong></p>
<p>Calculate premium tier value:</p>
<pre><code>Total premium articles: 10,000
Average uniqueness score: 8.5/10
Update frequency: 500 new articles/month
Industry: Financial data (high-value vertical)

Estimated licensing value: $500K-$1M/year
</code></pre>
<p><strong>Pricing worksheet:</strong> <a href="ai-training-data-pricing-publishers.html">ai-training-data-pricing-publishers.html</a></p>
<p><strong>3. Conversion optimization</strong></p>
<p>Improve free-to-premium conversion for AI companies.</p>
<p><strong>Tactics:</strong></p>
<ul>
<li><strong>Tease premium content</strong> (free tier articles reference premium research with licensing CTA)</li>
<li><strong>Sample premium articles</strong> (rotate 1-2 premium pieces to free tier monthly as showcase)</li>
<li><strong>Graduated access</strong> (first 5K requests to premium tier free, licensing required beyond)</li>
</ul>
<p><strong>A/B testing:</strong> Test different free tier sizes. Hypothesis: Smaller free tier (5K articles) generates same SEO benefit as larger (20K articles) but higher licensing revenue (scarcity increases perceived premium value).</p>
<h2>Strategy 3: Pay-to-Crawl Infrastructure</h2>
<h3>Technical Payment Authentication</h3>
<p><strong>Most sophisticated approach:</strong> Implement payment requirement directly in crawler access flow.</p>
<p><strong>Architecture:</strong></p>
<ol>
<li>Bot requests content</li>
<li>Server checks for payment authentication</li>
<li>If authenticated, serve content</li>
<li>If not authenticated, serve payment portal</li>
<li>Bot (or operator) completes payment</li>
<li>Server issues API key</li>
<li>Bot includes API key in subsequent requests</li>
</ol>
<p><strong>Payment-gated crawling flow:</strong></p>
<pre><code class="language-python">from flask import Flask, request, jsonify
import stripe

app = Flask(__name__)
stripe.api_key = &#39;sk_live_...&#39;

@app.route(&#39;/articles/&lt;article_id&gt;&#39;)
def serve_article(article_id):
    api_key = request.headers.get(&#39;X-API-Key&#39;)

    if api_key:
        # Verify API key and check payment status
        client = verify_api_key(api_key)

        if client and client.subscription_active:
            # Serve content
            article = fetch_article(article_id)

            # Meter usage
            meter_usage(client.id, &#39;article_access&#39;, 1)

            return jsonify({
                &#39;id&#39;: article.id,
                &#39;title&#39;: article.title,
                &#39;content&#39;: article.content,
                &#39;published_at&#39;: article.published_at
            })
        elif client and not client.subscription_active:
            return jsonify({&#39;error&#39;: &#39;Subscription expired&#39;}), 402

    # No API key or invalid key - serve payment portal
    return jsonify({
        &#39;error&#39;: &#39;Payment required&#39;,
        &#39;message&#39;: &#39;Content access requires active subscription&#39;,
        &#39;pricing&#39;: {
            &#39;monthly&#39;: &#39;$5,000/month - 100K requests&#39;,
            &#39;annual&#39;: &#39;$50,000/year - 1.5M requests&#39;,
            &#39;enterprise&#39;: &#39;Custom pricing - unlimited access&#39;
        },
        &#39;signup_url&#39;: &#39;https://yoursite.com/api-signup&#39;,
        &#39;docs_url&#39;: &#39;https://yoursite.com/api-docs&#39;
    }), 402  # Payment Required status
</code></pre>
<p><strong>API key generation after payment:</strong></p>
<pre><code class="language-python">@app.route(&#39;/api-signup&#39;, methods=[&#39;POST&#39;])
def api_signup():
    email = request.form.get(&#39;email&#39;)
    plan = request.form.get(&#39;plan&#39;)  # &#39;monthly&#39;, &#39;annual&#39;, &#39;enterprise&#39;

    # Create Stripe customer
    customer = stripe.Customer.create(
        email=email,
        metadata={&#39;plan&#39;: plan}
    )

    # Create subscription
    if plan == &#39;monthly&#39;:
        subscription = stripe.Subscription.create(
            customer=customer.id,
            items=[{&#39;price&#39;: &#39;price_monthly_5000&#39;}],
        )
    elif plan == &#39;annual&#39;:
        subscription = stripe.Subscription.create(
            customer=customer.id,
            items=[{&#39;price&#39;: &#39;price_annual_50000&#39;}],
        )

    # Generate API key
    api_key = generate_secure_api_key()

    # Store in database
    save_api_client({
        &#39;api_key&#39;: api_key,
        &#39;email&#39;: email,
        &#39;stripe_customer_id&#39;: customer.id,
        &#39;stripe_subscription_id&#39;: subscription.id,
        &#39;plan&#39;: plan,
        &#39;created_at&#39;: datetime.now()
    })

    # Send API key via email
    send_api_key_email(email, api_key)

    return jsonify({
        &#39;success&#39;: True,
        &#39;api_key&#39;: api_key,
        &#39;docs_url&#39;: &#39;https://yoursite.com/api-docs&#39;
    })
</code></pre>
<p><strong>Automated enforcement:</strong> No API key = no content. Payment lapses (subscription expires) → API key deactivated → access revoked automatically.</p>
<h3>Usage Metering and Billing</h3>
<p><strong>Track consumption, bill accordingly.</strong></p>
<p><strong>Per-request metering:</strong></p>
<pre><code class="language-python">def meter_usage(client_id, metric_name, quantity):
    &quot;&quot;&quot;Record usage event for billing&quot;&quot;&quot;

    # Increment usage counter (Redis)
    key = f&quot;usage:{client_id}:{datetime.now().strftime(&#39;%Y-%m&#39;)}&quot;
    redis.hincrby(key, metric_name, quantity)
    redis.expire(key, 90 * 86400)  # Retain 90 days

    # Log to data warehouse (Snowflake, BigQuery) for analytics
    log_usage_event({
        &#39;client_id&#39;: client_id,
        &#39;metric&#39;: metric_name,
        &#39;quantity&#39;: quantity,
        &#39;timestamp&#39;: datetime.now(),
        &#39;metadata&#39;: {
            &#39;article_id&#39;: request.view_args.get(&#39;article_id&#39;),
            &#39;user_agent&#39;: request.headers.get(&#39;User-Agent&#39;)
        }
    })
</code></pre>
<p><strong>Billing job (monthly):</strong></p>
<pre><code class="language-python">def generate_monthly_invoices():
    &quot;&quot;&quot;Create invoices for usage-based billing&quot;&quot;&quot;

    billing_month = last_month()

    for client in get_all_api_clients():
        usage_key = f&quot;usage:{client.id}:{billing_month}&quot;
        usage_data = redis.hgetall(usage_key)

        # Calculate charges
        article_requests = int(usage_data.get(&#39;article_access&#39;, 0))

        if client.plan == &#39;monthly&#39;:
            base_fee = 5000  # $5,000 base
            included_requests = 100000
            overage_rate = 0.06  # $0.06 per request

            overage_requests = max(0, article_requests - included_requests)
            overage_charges = overage_requests * overage_rate

            total_amount = base_fee + overage_charges

        # Create Stripe invoice
        invoice = stripe.InvoiceItem.create(
            customer=client.stripe_customer_id,
            amount=int(total_amount * 100),  # Cents
            currency=&#39;usd&#39;,
            description=f&#39;API Usage - {billing_month}&#39;
        )

        # Finalize and charge
        stripe.Invoice.create(
            customer=client.stripe_customer_id,
            auto_advance=True  # Auto-charge
        )

        # Send usage report email
        send_usage_report(client, {
            &#39;requests&#39;: article_requests,
            &#39;base_fee&#39;: base_fee,
            &#39;overage_charges&#39;: overage_charges,
            &#39;total&#39;: total_amount,
            &#39;billing_month&#39;: billing_month
        })
</code></pre>
<p><strong>Real-time billing visibility:</strong> Customer dashboard showing current month usage, projected charges.</p>
<pre><code class="language-python">@app.route(&#39;/api-dashboard&#39;)
def api_dashboard():
    api_key = request.headers.get(&#39;X-API-Key&#39;)
    client = verify_api_key(api_key)

    # Fetch current month usage
    usage_key = f&quot;usage:{client.id}:{datetime.now().strftime(&#39;%Y-%m&#39;)}&quot;
    usage_data = redis.hgetall(usage_key)

    article_requests = int(usage_data.get(&#39;article_access&#39;, 0))

    # Calculate projected charges
    if client.plan == &#39;monthly&#39;:
        base = 5000
        included = 100000
        overage_rate = 0.06

        projected_overage = max(0, article_requests - included) * overage_rate
        projected_total = base + projected_overage

    return render_template(&#39;api_dashboard.html&#39;,
        current_requests=article_requests,
        included_requests=included,
        overage_requests=max(0, article_requests - included),
        projected_total=projected_total,
        days_remaining=days_until_month_end()
    )
</code></pre>
<p><strong>Transparency reduces billing disputes.</strong> Client monitors usage in real-time, adjusts scraping behavior to control costs.</p>
<h3>Integration With Cloudflare Pay-Per-Crawl</h3>
<p><strong>Cloudflare offers built-in pay-per-crawl infrastructure.</strong> Simplifies implementation for publishers using Cloudflare CDN.</p>
<p><strong>Setup process:</strong></p>
<ol>
<li><strong>Enable Bot Management</strong> (Cloudflare dashboard → Security → Bots)</li>
<li><strong>Configure AI Crawler Settings</strong> (set pricing per request)</li>
<li><strong>Connect Stripe account</strong> (revenue payout destination)</li>
<li><strong>Set access rules</strong> (which bots allowed, blocked, or paywalled)</li>
</ol>
<p><strong>Cloudflare handles:</strong></p>
<ul>
<li>Bot detection and verification</li>
<li>Payment processing (Stripe integration)</li>
<li>Access enforcement (blocks unpaid bots)</li>
<li>Revenue distribution (deposits to your Stripe account)</li>
</ul>
<p><strong>Pricing configuration:</strong></p>
<pre><code># Cloudflare dashboard config
AI Crawler Pricing:
  GPTBot: $0.01 per request
  ClaudeBot: $0.01 per request
  Gemini: $0.01 per request
  Other AI crawlers: $0.02 per request
</code></pre>
<p><strong>Revenue share:</strong> Cloudflare takes platform fee (estimated 20-30%). Example:</p>
<ul>
<li><strong>Bot requests:</strong> 50,000/month</li>
<li><strong>Rate:</strong> $0.01/request</li>
<li><strong>Gross revenue:</strong> $500</li>
<li><strong>Cloudflare fee (25%):</strong> $125</li>
<li><strong>Publisher net:</strong> $375/month = $4,500/year</li>
</ul>
<p><strong>Comparison to custom implementation:</strong></p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>Custom Solution</th>
<th>Cloudflare Pay-Per-Crawl</th>
</tr>
</thead>
<tbody><tr>
<td>Setup time</td>
<td>40-80 hours dev</td>
<td>30 minutes config</td>
</tr>
<tr>
<td>Technical complexity</td>
<td>High (API, billing, auth)</td>
<td>Low (dashboard toggle)</td>
</tr>
<tr>
<td>Revenue share</td>
<td>100%</td>
<td>70-80% (platform fee)</td>
</tr>
<tr>
<td>Bot coverage</td>
<td>Custom (add new bots manually)</td>
<td>Automatic (Cloudflare updates)</td>
</tr>
<tr>
<td>Enforcement reliability</td>
<td>Depends on implementation</td>
<td>High (Cloudflare infrastructure)</td>
</tr>
</tbody></table>
<p><strong>Best for:</strong> Publishers lacking engineering resources. Trade lower revenue (platform fee) for zero technical overhead.</p>
<p><strong>Hybrid strategy:</strong> Use Cloudflare for mainstream bots (GPTBot, ClaudeBot). Negotiate direct licensing with high-volume customers (OpenAI enterprise license).</p>
<p><strong>Integration guide:</strong> <a href="cloudflare-pay-per-crawl-setup.html">cloudflare-pay-per-crawl-setup.html</a></p>
<h2>Hybrid Paywall Strategies</h2>
<h3>Combining Selective Gating With Freemium</h3>
<p><strong>Multi-tier access architecture:</strong></p>
<p><strong>Tier 1: Free (search engines)</strong></p>
<ul>
<li>Googlebot: Full access (SEO priority)</li>
<li>Bingbot: Full access</li>
</ul>
<p><strong>Tier 2: Freemium (AI training bots - free content only)</strong></p>
<ul>
<li>GPTBot: Access to /news/, /archive/</li>
<li>ClaudeBot: Access to /news/, /archive/</li>
<li>Bytespider: Blocked entirely (compliance issues)</li>
</ul>
<p><strong>Tier 3: Premium (licensed AI bots)</strong></p>
<ul>
<li>OpenAI (licensed): Full access including /premium/, /research/</li>
<li>Anthropic (licensed): Full access</li>
</ul>
<p><strong>Tier 4: Pay-per-crawl (unknown bots)</strong></p>
<ul>
<li>Unknown crawlers: API authentication required</li>
</ul>
<p><strong>Implementation (nginx):</strong></p>
<pre><code class="language-nginx">map $http_user_agent $bot_tier {
    default &quot;unknown&quot;;
    ~*Googlebot &quot;search_engine&quot;;
    ~*Bingbot &quot;search_engine&quot;;
    ~*GPTBot &quot;ai_freemium&quot;;
    ~*ClaudeBot &quot;ai_freemium&quot;;
    ~*Bytespider &quot;blocked&quot;;
}

# Licensed bots (API key auth)
map $http_x_api_key $licensed_bot {
    default 0;
    &quot;sk_openai_...&quot; 1;
    &quot;sk_anthropic_...&quot; 1;
}

location / {
    # Block tier
    if ($bot_tier = &quot;blocked&quot;) {
        return 403;
    }

    # Freemium tier - restrict to free content
    if ($bot_tier = &quot;ai_freemium&quot;) {
        # Only allow /news/ and /archive/
        if ($uri !~ &quot;^/(news|archive)/&quot;) {
            return 403 &quot;Premium content requires licensing&quot;;
        }
    }

    # Licensed bots - full access
    if ($licensed_bot = 1) {
        proxy_pass http://backend;
        break;
    }

    # Search engines - full access
    if ($bot_tier = &quot;search_engine&quot;) {
        proxy_pass http://backend;
        break;
    }

    # Unknown bots - require API auth
    if ($bot_tier = &quot;unknown&quot;) {
        return 402 &quot;API key required&quot;;
    }

    # Humans - standard paywall
    proxy_pass http://backend;
}
</code></pre>
<p><strong>Revenue optimization:</strong></p>
<ul>
<li><strong>Search engines:</strong> Drive $X in SEO traffic value</li>
<li><strong>AI freemium tier:</strong> Generate awareness (value hard to quantify but real)</li>
<li><strong>Licensed bots:</strong> $Y annual licensing fees</li>
<li><strong>Pay-per-crawl:</strong> $Z from unknown crawlers</li>
</ul>
<p><strong>Total value = SEO + Licensing + Pay-per-crawl revenue</strong></p>
<h3>Progressive Licensing Incentives</h3>
<p><strong>Encourage AI companies to upgrade from freemium to licensed tiers.</strong></p>
<p><strong>Graduated access model:</strong></p>
<p><strong>Month 1: Free tier (10K requests)</strong></p>
<ul>
<li>Limited to /news/ and /archive/</li>
<li>No support, no SLA</li>
</ul>
<p><strong>Month 2-3: Trial license (50K requests)</strong></p>
<ul>
<li>Include /premium/ content</li>
<li>Email support</li>
<li><strong>Cost:</strong> $1,000/month trial rate</li>
</ul>
<p><strong>Month 4+: Full license</strong></p>
<ul>
<li>Unlimited requests</li>
<li>Full archive access</li>
<li>API access, dedicated support</li>
<li><strong>Cost:</strong> $10,000/month standard rate</li>
</ul>
<p><strong>Incentive structure:</strong></p>
<p>&quot;Access first 10K requests free. Demonstrates content value. To continue beyond quota, enter trial license ($1K/month). After 2 months trial, upgrade to full license with volume discount.&quot;</p>
<p><strong>Implementation:</strong></p>
<pre><code class="language-python">@app.before_request
def progressive_licensing_gate():
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    if not is_training_bot(user_agent):
        return None

    bot_id = identify_bot(user_agent)

    # Check current usage tier
    usage = get_bot_monthly_usage(bot_id)
    license_status = get_bot_license_status(bot_id)

    if license_status == &#39;full_license&#39;:
        return None  # Full access
    elif license_status == &#39;trial_license&#39;:
        if usage &gt; 50000:
            return render_template(&#39;trial_limit_reached.html&#39;), 402
        return None  # Allow access
    elif license_status == &#39;free_tier&#39;:
        if usage &gt; 10000:
            return render_template(&#39;free_limit_reached.html&#39;), 402

        # Restrict to free content
        if not request.path.startswith((&#39;/news/&#39;, &#39;/archive/&#39;)):
            return render_template(&#39;premium_requires_trial.html&#39;), 403

        return None
    else:
        # No license - offer free tier
        return render_template(&#39;licensing_tiers.html&#39;), 402
</code></pre>
<p><strong>Conversion funnel:</strong></p>
<ol>
<li><strong>Awareness:</strong> Bot accesses free tier (learns about content)</li>
<li><strong>Engagement:</strong> Exceeds free quota (demonstrates value)</li>
<li><strong>Trial:</strong> Upgrades to trial license (tests premium content)</li>
<li><strong>Conversion:</strong> Upgrades to full license (committed customer)</li>
</ol>
<p><strong>Optimization:</strong> Track conversion rates at each stage. If free → trial is low (e.g., 5%), increase free tier quota (20K instead of 10K). If trial → full is low, offer discount (&quot;Upgrade now, get 20% off first year&quot;).</p>
<h3>Cross-Platform Paywall Coordination</h3>
<p><strong>Publishers operate multiple properties.</strong> Coordinate paywall strategy across portfolio.</p>
<p><strong>Example portfolio:</strong></p>
<ul>
<li><strong>Site A:</strong> Main news site (50M monthly visitors)</li>
<li><strong>Site B:</strong> Industry research vertical (5M visitors)</li>
<li><strong>Site C:</strong> Newsletter archive site (1M visitors)</li>
</ul>
<p><strong>Uncoordinated paywalls:</strong> AI company scrapes Site C (weakest technical defenses), obtains content available on Site A (behind stronger paywall). <strong>Enforcement failure.</strong></p>
<p><strong>Coordinated strategy:</strong></p>
<p><strong>1. Unified licensing terms</strong></p>
<p>Single licensing agreement covers all properties.</p>
<pre><code>LICENSE SCOPE

This Agreement grants Licensee access to Publisher&#39;s content across:
- NewsSite.com (main publication)
- ResearchSite.com (industry analysis)
- NewsletterArchive.com (subscriber communications)

Licensee shall access content via unified API endpoint: api.publisher.com
</code></pre>
<p><strong>2. Shared authentication</strong></p>
<p>API key works across all properties.</p>
<pre><code class="language-python"># Central auth service (auth.publisher.com)
@app.route(&#39;/verify-api-key&#39;, methods=[&#39;POST&#39;])
def verify_api_key():
    api_key = request.json.get(&#39;api_key&#39;)
    domain = request.json.get(&#39;domain&#39;)

    client = lookup_api_client(api_key)

    if client and client.subscription_active:
        # Check if license covers requested domain
        if domain in client.licensed_domains:
            return jsonify({&#39;authorized&#39;: True, &#39;client_id&#39;: client.id})

    return jsonify({&#39;authorized&#39;: False}), 403

# Each property checks auth via central service
@app.before_request  # On Site A, B, C
def check_authorization():
    api_key = request.headers.get(&#39;X-API-Key&#39;)

    if api_key:
        response = requests.post(&#39;https://auth.publisher.com/verify-api-key&#39;,
            json={&#39;api_key&#39;: api_key, &#39;domain&#39;: request.host})

        if response.json().get(&#39;authorized&#39;):
            return None  # Authorized

    # Not authorized
    return render_template(&#39;licensing_required.html&#39;), 403
</code></pre>
<p><strong>3. Consistent pricing</strong></p>
<p>Portfolio licensing more expensive than single-site but offers volume discount.</p>
<table>
<thead>
<tr>
<th>Scope</th>
<th>Price</th>
</tr>
</thead>
<tbody><tr>
<td>Site A only</td>
<td>$100K/year</td>
</tr>
<tr>
<td>Site B only</td>
<td>$50K/year</td>
</tr>
<tr>
<td>Site C only</td>
<td>$20K/year</td>
</tr>
<tr>
<td><strong>Portfolio (all 3)</strong></td>
<td><strong>$150K/year</strong> (12% discount vs. $170K sum)</td>
</tr>
</tbody></table>
<p><strong>Cross-platform enforcement prevents arbitrage</strong> (scraping cheaper property to access content available on expensive property).</p>
<h2>Performance and Monitoring</h2>
<h3>Tracking Bot Access Patterns</h3>
<p><strong>Visibility into crawler behavior informs enforcement decisions.</strong></p>
<p><strong>Log collection:</strong></p>
<pre><code class="language-python">@app.after_request
def log_crawler_access(response):
    user_agent = request.headers.get(&#39;User-Agent&#39;, &#39;&#39;)

    if is_bot(user_agent):
        log_entry = {
            &#39;timestamp&#39;: datetime.now(),
            &#39;user_agent&#39;: user_agent,
            &#39;bot_type&#39;: identify_bot(user_agent),
            &#39;ip_address&#39;: request.remote_addr,
            &#39;url&#39;: request.url,
            &#39;status_code&#39;: response.status_code,
            &#39;bytes_transferred&#39;: len(response.get_data()),
            &#39;api_key&#39;: request.headers.get(&#39;X-API-Key&#39;, None)
        }

        # Write to data warehouse
        bigquery_client.insert_rows(&#39;crawler_access_logs&#39;, [log_entry])

    return response
</code></pre>
<p><strong>Analytics queries:</strong></p>
<p><strong>Top crawlers by request volume:</strong></p>
<pre><code class="language-sql">SELECT
    bot_type,
    COUNT(*) as requests,
    SUM(bytes_transferred) as total_bytes,
    COUNT(DISTINCT ip_address) as unique_ips
FROM crawler_access_logs
WHERE timestamp &gt;= CURRENT_DATE - INTERVAL &#39;30 days&#39;
GROUP BY bot_type
ORDER BY requests DESC;
</code></pre>
<p><strong>Content access patterns:</strong></p>
<pre><code class="language-sql">SELECT
    bot_type,
    CASE
        WHEN url LIKE &#39;%/premium/%&#39; THEN &#39;premium&#39;
        WHEN url LIKE &#39;%/research/%&#39; THEN &#39;research&#39;
        WHEN url LIKE &#39;%/news/%&#39; THEN &#39;news&#39;
        ELSE &#39;other&#39;
    END as content_tier,
    COUNT(*) as requests
FROM crawler_access_logs
WHERE timestamp &gt;= CURRENT_DATE - INTERVAL &#39;7 days&#39;
GROUP BY bot_type, content_tier
ORDER BY bot_type, requests DESC;
</code></pre>
<p><strong>Blocked access attempts:</strong></p>
<pre><code class="language-sql">SELECT
    bot_type,
    ip_address,
    COUNT(*) as blocked_attempts,
    ARRAY_AGG(DISTINCT url LIMIT 10) as attempted_urls
FROM crawler_access_logs
WHERE status_code IN (403, 402)
  AND timestamp &gt;= CURRENT_DATE - INTERVAL &#39;7 days&#39;
GROUP BY bot_type, ip_address
HAVING COUNT(*) &gt; 100  -- Persistent violators
ORDER BY blocked_attempts DESC;
</code></pre>
<p><strong>Dashboards (Grafana, Looker):</strong></p>
<ul>
<li><strong>Real-time crawler activity</strong> (requests/minute by bot type)</li>
<li><strong>Paywall effectiveness</strong> (block rate, licensing conversion rate)</li>
<li><strong>Revenue attribution</strong> (requests by licensed vs. unlicensed bots)</li>
</ul>
<p><strong>Alerting:</strong> Notify when unusual activity detected (sudden spike in blocked requests, new unknown crawler, licensed bot exceeding quota).</p>
<h3>Revenue Attribution and ROI</h3>
<p><strong>Measure paywall strategy financial impact.</strong></p>
<p><strong>Revenue sources:</strong></p>
<ol>
<li><strong>Licensing fees</strong> (direct)</li>
<li><strong>API subscription revenue</strong> (direct)</li>
<li><strong>Reduced scraping costs</strong> (indirect - lower bandwidth from blocking)</li>
<li><strong>Attribution traffic value</strong> (indirect - referrals from licensed bots)</li>
</ol>
<p><strong>Calculation example:</strong></p>
<p><strong>Costs:</strong></p>
<ul>
<li>Development time: 80 hours × $150/hr = $12,000</li>
<li>Infrastructure: $500/month (auth service, monitoring) = $6,000/year</li>
<li>Maintenance: 10 hours/month × $150/hr = $18,000/year</li>
<li><strong>Total annual cost:</strong> $36,000</li>
</ul>
<p><strong>Revenue:</strong></p>
<ul>
<li>Licensing deals: 3 AI companies × $100K avg = $300,000/year</li>
<li>API subscriptions: 5 clients × $5K/month × 12 = $300,000/year</li>
<li>Bandwidth savings: 80% reduction in unlicensed scraping = $10,000/year</li>
<li>Attribution traffic: Referrals from licensed bots generate $50,000 ad revenue</li>
<li><strong>Total annual revenue:</strong> $660,000</li>
</ul>
<p><strong>ROI:</strong> ($660K - $36K) / $36K = 1,733%</p>
<p><strong>Payback period:</strong> ~20 days (recovered investment in first month)</p>
<p><strong>Sensitivity analysis:</strong> Revenue projections depend on licensing success rate. Conservative scenario (only 1 deal signed, $100K/year): ROI = 178%. Optimistic scenario (5 deals, $1M total): ROI = 2,678%.</p>
<h3>Continuous Optimization</h3>
<p><strong>Paywall strategies require iteration.</strong></p>
<p><strong>Monthly review cycle:</strong></p>
<p><strong>Week 1: Data collection</strong></p>
<ul>
<li>Export crawler access logs</li>
<li>Survey licensing pipeline (active negotiations)</li>
<li>Measure enforcement effectiveness (block rate, false positives)</li>
</ul>
<p><strong>Week 2: Analysis</strong></p>
<ul>
<li>Identify patterns (which bots scraping most, which content accessed)</li>
<li>Revenue attribution (licensing fees vs. attribution traffic value)</li>
<li>Cost analysis (bandwidth savings from blocking)</li>
</ul>
<p><strong>Week 3: Optimization</strong></p>
<ul>
<li>Adjust free tier quotas (increase/decrease based on licensing conversion)</li>
<li>Pricing experiments (test different licensing rates)</li>
<li>Technical improvements (reduce false positives, improve bot detection)</li>
</ul>
<p><strong>Week 4: Implementation</strong></p>
<ul>
<li>Deploy changes</li>
<li>Update documentation (API docs, licensing pages)</li>
<li>Communicate changes to licensed partners</li>
</ul>
<p><strong>A/B testing framework:</strong></p>
<p>Test paywall variations on different bot types.</p>
<p><strong>Test:</strong> Increase free tier quota from 10K to 20K requests/month for GPTBot. <strong>Hypothesis:</strong> Higher quota drives more trial signups (bot operator evaluates content more thoroughly before committing to license).</p>
<p><strong>Measurement:</strong> Compare licensing conversion rate (free tier → paid license) between 10K control group and 20K test group.</p>
<p><strong>Iterate based on data.</strong> If test group converts 2× better, roll out 20K quota to all bots. If no difference, revert (no benefit, just increased scraping cost).</p>
<h2>FAQ</h2>
<h3>How do I prevent AI companies from scraping my paywalled content?</h3>
<p><strong>Multi-layer enforcement:</strong> (1) <strong>robots.txt</strong> declares intent (block AI training bots), (2) <strong>IP verification</strong> prevents user agent spoofing (reverse DNS confirms bot identity), (3) <strong>Server-level blocks</strong> enforce robots.txt directives (nginx/Apache rules returning 403 for training bots), (4) <strong>API authentication</strong> gates premium content (requires valid API key obtained via licensing), (5) <strong>Legal terms</strong> provide enforcement mechanism (Terms of Service prohibit unauthorized AI training, enables litigation if violations persist). <strong>No single layer is foolproof.</strong> Combination creates friction making scraping expensive enough to incentivize licensing instead. Technical blocks work best when paired with clear licensing pathway (make legal path easier than circumvention).</p>
<h3>What&#39;s the difference between blocking and gating AI crawlers?</h3>
<p><strong>Blocking</strong> = absolute denial (crawler receives 403 error, zero access). <strong>Gating</strong> = conditional access (crawler can access content after meeting conditions—payment, authentication, licensing agreement). <strong>Blocking strategy:</strong> Protects content from all training use. Zero revenue but maximum control. <strong>Gating strategy:</strong> Monetizes crawler access. Content becomes revenue-generating asset. <strong>Hybrid approach:</strong> Block default (training bots denied), gate with licensing option (provide path to pay for access). Publishers pursuing monetization use gating. Publishers prioritizing IP protection (no willingness to license at any price) use blocking.</p>
<h3>Can AI companies bypass my paywall by using residential proxies?</h3>
<p><strong>Yes, but costly and detectable.</strong> Residential proxies rotate IP addresses (appear as human users from homes/mobile devices). Bypasses IP-based blocking. <strong>Countermeasures:</strong> (1) <strong>Behavioral analysis</strong> (bots exhibit patterns—sequential URL access, no mouse movements, fast page transitions—that differ from humans), (2) <strong>Honeypots</strong> (invisible links only bots follow, flag IP as crawler), (3) <strong>Rate limiting</strong> (even residential proxies can&#39;t realistically access 10K pages/day from single account without triggering anomaly detection), (4) <strong>Challenge-response</strong> (CAPTCHAs, proof-of-work challenges uneconomical for large-scale scraping). <strong>Scale matters:</strong> Small-scale proxy scraping might succeed. Large-scale training data collection (millions of pages) becomes expensive enough to make licensing competitive. Enforcement isn&#39;t about perfect defense—it&#39;s about making circumvention more expensive than compliance.</p>
<h3>Should I offer free access to AI search engines like Perplexity?</h3>
<p><strong>Depends on attribution behavior.</strong> <strong>Perplexity</strong> uses your content to generate answers (synthesis). <strong>Critical question:</strong> Do they cite your site with clickable links? <strong>If yes:</strong> Consider allowing access (attribution drives referral traffic, similar to Google search). Measure referral value (visits × pages/visit × ad revenue per visit). If referral revenue &gt; scraping costs, allow access. <strong>If no:</strong> Block or gate access. No referral traffic = pure extraction (they monetize your content via subscriptions, you get nothing). <strong>Negotiation leverage:</strong> &quot;We&#39;ll allow PerplexityBot access if attribution citations are guaranteed in licensing agreement. Without attribution, blocking remains in effect.&quot; AI search engines building traffic on publisher content owe reciprocal value (traffic referrals or licensing fees).</p>
<h3>What licensing model generates the most revenue for mid-size publishers?</h3>
<p><strong>Flat-fee annual licensing with usage quotas</strong> typically maximizes revenue for mid-size publishers (1M-10M monthly visitors). <strong>Structure:</strong> Base fee ($100K-$500K/year depending on content value) + included quota (200K-500K requests/month) + overage charges ($0.01-$0.05 per request beyond quota). <strong>Why this works:</strong> (1) <strong>Predictable base revenue</strong> (guaranteed annual income regardless of AI company usage fluctuations), (2) <strong>Upside from heavy use</strong> (overage charges capture value if scraping exceeds expectations), (3) <strong>Simple pricing</strong> (easier to negotiate than complex usage tiers), (4) <strong>Scalable</strong> (license to multiple AI companies at similar rates). <strong>Alternative for high-value niche publishers:</strong> Usage-based pricing can outperform flat-fee if content is mission-critical to AI company (financial data, medical research, legal case law). Example: Financial data provider charges per-token ($0.001 per 1K tokens). AI company training finance-focused model uses 100M tokens = $100K. High usage → higher revenue than flat-fee.</p>

      </article>

      <div class="cta-box" style="margin: var(--sp-16) 0;">
        <h3>Your Content Feeds AI. Get Paid for It.</h3>
        <p>Complete pay-per-crawl implementation. Templates, pricing models, licensing contracts. Everything.</p>
        <a href="/setup.html" class="btn btn--primary btn--large">Master the Protocol &mdash; $2,497</a>
      </div>

      <div style="margin-top: var(--sp-8); padding-top: var(--sp-8); border-top: 1px solid var(--border);">
        <a href="/articles.html" style="font-family: var(--font-mono); font-size: 0.875rem; font-weight: 500; color: var(--accent);">&larr; All Articles</a>
      </div>
    </div>
  </main>


  <footer class="footer">
    <div class="container">
      <div class="footer__grid">
        <div class="footer__brand">
          <div class="footer__logo">AI Pay Per Crawl</div>
          <p class="footer__desc">Your content feeds AI. The protocol ensures you get paid for it. Implementation guides, pricing models, and licensing infrastructure for publishers monetizing AI crawler traffic.</p>
          <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
        </div>

        <div>
          <div class="footer__heading">Implementation</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Cloudflare Setup</a></li>
            <li><a href="/articles.html">RSL Protocol</a></li>
            <li><a href="/articles.html">llms.txt Guide</a></li>
            <li><a href="/articles.html">Nginx Rules</a></li>
            <li><a href="/articles.html">Apache Config</a></li>
            <li><a href="/articles.html">WordPress Plugin</a></li>
            <li><a href="/articles.html">CDN Integration</a></li>
            <li><a href="/articles.html">robots.txt Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">Pricing &amp; Tools</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Pricing Models</a></li>
            <li><a href="/articles.html">Rate Cards</a></li>
            <li><a href="/articles.html">Revenue Calculator</a></li>
            <li><a href="/articles.html">Crawler Directory</a></li>
            <li><a href="/articles.html">Bot Detection</a></li>
            <li><a href="/articles.html">Compliance Rates</a></li>
            <li><a href="/articles.html">Deal Analysis</a></li>
            <li><a href="/articles.html">Licensing Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">From Scale With Search</div>
          <ul class="footer__links">
            <li><a href="https://scalewithsearch.com" target="_blank" rel="me">Scale With Search</a></li>
            <li><a href="https://aifirstsearch.com" target="_blank" rel="me">AI First Search</a></li>
            <li><a href="https://browserprompt.com" target="_blank" rel="me">Browser Prompt</a></li>
            <li><a href="https://polytraffic.com" target="_blank" rel="me">Polytraffic</a></li>
            <li><a href="https://creatinepedia.com" target="_blank" rel="me">Creatinepedia</a></li>
            <li><a href="https://victorvalentineromo.com" target="_blank" rel="me">Victor Romo</a></li>
            <li><a href="https://b2bvic.com" target="_blank" rel="me">B2B Vic</a></li>
          </ul>
        </div>
      </div>

      <div class="footer__bar">
        <p>&copy; 2026 AI Pay Per Crawl. Built by <a href="https://victorvalentineromo.com" target="_blank">Victor Valentine Romo</a>.</p>
        <p>A <a href="https://scalewithsearch.com" target="_blank">Scale With Search</a> property.</p>
      </div>
    </div>
  </footer>


  <script>
    /* Mega menu */
    (function() {
      var triggers = document.querySelectorAll('.mega-trigger');
      var panels = document.querySelectorAll('.mega-panel');
      var closeTimeout = null;

      function openPanel(id) {
        clearTimeout(closeTimeout);
        panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
        triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        var panel = document.getElementById('mega-' + id);
        var trigger = document.querySelector('[data-mega="' + id + '"] .mega-trigger');
        if (panel) panel.classList.add('mega-panel--open');
        if (trigger) trigger.setAttribute('aria-expanded', 'true');
      }

      function scheduleClose() {
        closeTimeout = setTimeout(function() {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }, 200);
      }

      function cancelClose() { clearTimeout(closeTimeout); }

      triggers.forEach(function(btn) {
        var wrapper = btn.closest('[data-mega]');
        var id = wrapper.getAttribute('data-mega');
        btn.addEventListener('mouseenter', function() { openPanel(id); });
        btn.addEventListener('click', function(e) {
          e.preventDefault();
          var panel = document.getElementById('mega-' + id);
          if (panel && panel.classList.contains('mega-panel--open')) {
            panel.classList.remove('mega-panel--open');
            btn.setAttribute('aria-expanded', 'false');
          } else {
            openPanel(id);
          }
        });
        wrapper.addEventListener('mouseleave', scheduleClose);
      });

      panels.forEach(function(panel) {
        panel.addEventListener('mouseenter', cancelClose);
        panel.addEventListener('mouseleave', scheduleClose);
      });

      document.addEventListener('click', function(e) {
        if (!e.target.closest('[data-mega]') && !e.target.closest('.mega-panel')) {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });

      document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape') {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });
    })();

    /* Mobile menu */
    var mobileMenuBtn = document.getElementById('mobile-menu-btn');
    var mobileMenu = document.getElementById('mobile-menu');
    var menuIcon = document.getElementById('menu-icon');
    var closeIcon = document.getElementById('close-icon');
    if (mobileMenuBtn) {
      mobileMenuBtn.addEventListener('click', function() {
        mobileMenu.classList.toggle('mobile-menu--open');
        menuIcon.classList.toggle('hidden');
        closeIcon.classList.toggle('hidden');
      });
    }

    /* Self-link neutralizer */
    (function() {
      var currentPath = window.location.pathname.replace(/\.html$/, '').replace(/\/$/, '') || '/';
      document.querySelectorAll('a[href]').forEach(function(a) {
        var href = a.getAttribute('href').replace(/\.html$/, '').replace(/\/$/, '') || '/';
        if (href === currentPath) {
          a.removeAttribute('href');
          a.setAttribute('aria-current', 'page');
          a.style.opacity = '0.5';
          a.style.pointerEvents = 'none';
        }
      });
    })();

    /* Copy code button */
    function copyCode(button) {
      var codeBlock = button.closest('.code-block');
      var code = codeBlock.querySelector('code').innerText;
      navigator.clipboard.writeText(code).then(function() {
        button.textContent = 'Copied!';
        setTimeout(function() { button.textContent = 'Copy'; }, 2000);
      });
    }

    /* FAQ Accordion */
    document.querySelectorAll('.accordion__trigger').forEach(function(trigger) {
      trigger.addEventListener('click', function() {
        var expanded = this.getAttribute('aria-expanded') === 'true';
        var body = this.nextElementSibling;
        this.setAttribute('aria-expanded', !expanded);
        body.classList.toggle('accordion__body--open');
      });
    });
  </script>
</body>
</html>