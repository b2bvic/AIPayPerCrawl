<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide | AI Pay Per Crawl</title>
    <meta name="description" content="Build production-grade AI crawler monitoring infrastructure using Prometheus metrics and Grafana dashboards. Tracks GPTBot, CCBot, ClaudeBot bandwidth, compliance, and anomaly detection.">
    <meta name="author" content="Victor Valentine Romo">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide">
    <meta property="og:description" content="Build production-grade AI crawler monitoring infrastructure using Prometheus metrics and Grafana dashboards. Tracks GPTBot, CCBot, ClaudeBot bandwidth, compliance, and anomaly detection.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://aipaypercrawl.com/articles/prometheus-grafana-ai-crawler-metrics">
    <meta property="og:site_name" content="AI Pay Per Crawl">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide">
    <meta name="twitter:description" content="Build production-grade AI crawler monitoring infrastructure using Prometheus metrics and Grafana dashboards. Tracks GPTBot, CCBot, ClaudeBot bandwidth, compliance, and anomaly detection.">
    <link rel="canonical" href="https://aipaypercrawl.com/articles/prometheus-grafana-ai-crawler-metrics">
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "TechArticle",
  "headline": "Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide",
  "description": "Build production-grade AI crawler monitoring infrastructure using Prometheus metrics and Grafana dashboards. Tracks GPTBot, CCBot, ClaudeBot bandwidth, compliance, and anomaly detection.",
  "author": {
    "@type": "Person",
    "@id": "https://victorvalentineromo.com/#person",
    "name": "Victor Valentine Romo",
    "url": "https://victorvalentineromo.com"
  },
  "publisher": {
    "@type": "Organization",
    "@id": "https://aipaypercrawl.com/#organization",
    "name": "AI Pay Per Crawl",
    "url": "https://aipaypercrawl.com"
  },
  "datePublished": "2026-02-08",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aipaypercrawl.com/articles/prometheus-grafana-ai-crawler-metrics"
  }
}
    </script>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://aipaypercrawl.com"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Articles",
      "item": "https://aipaypercrawl.com/articles.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide",
      "item": "https://aipaypercrawl.com/articles/prometheus-grafana-ai-crawler-metrics"
    }
  ]
}
    </script>

    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect fill='%234f46e5' width='100' height='100' rx='12'/><text x='50' y='70' font-family='monospace' font-size='38' font-weight='700' fill='%23ffffff' text-anchor='middle'>APC</text></svg>">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet"></noscript>
    <link rel="stylesheet" href="/base.css">
    <link rel="me" href="https://scalewithsearch.com">
    <link rel="me" href="https://victorvalentineromo.com">
    <link rel="me" href="https://aifirstsearch.com">
    <link rel="me" href="https://browserprompt.com">
    <link rel="me" href="https://creatinepedia.com">
    <link rel="me" href="https://polytraffic.com">
    <link rel="me" href="https://tattooremovalnear.com">
    <link rel="me" href="https://comicstripai.com">
    <link rel="me" href="https://aipaypercrawl.com">
    <link rel="me" href="https://b2bvic.com">
    <link rel="me" href="https://seobyrole.com">
    <link rel="me" href="https://quickfixseo.com">
</head>
<body>


  <nav class="nav" role="navigation" aria-label="Primary">
    <div class="nav__inner">
      <a href="/" class="nav__logo">APC</a>

      <div class="nav__links">
        <!-- Implementation dropdown -->
        <div class="mega-wrapper" data-mega="implementation">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-implementation">
            Implementation
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Pricing dropdown -->
        <div class="mega-wrapper" data-mega="pricing">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-pricing">
            Pricing
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Crawlers dropdown -->
        <div class="mega-wrapper" data-mega="crawlers">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-crawlers">
            Crawlers
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Legal (direct link) -->
        <a href="/articles.html" class="nav__link">Legal</a>

        <div class="nav__divider"></div>

        <a href="/setup.html" class="nav__cta">Master the Protocol &mdash; $2,497</a>
      </div>

      <button class="nav__mobile-btn" id="mobile-menu-btn" aria-label="Open menu">
        <svg id="menu-icon" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
        <svg id="close-icon" class="hidden" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
      </button>
    </div>

    <!-- Mega Panel: Implementation -->
    <div id="mega-implementation" class="mega-panel" role="region" aria-label="Implementation guides">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Infrastructure</div>
          <a href="/articles.html" class="mega-link">Cloudflare Setup</a>
          <a href="/articles.html" class="mega-link">Nginx Rules</a>
          <a href="/articles.html" class="mega-link">Apache Config</a>
          <a href="/articles.html" class="mega-link">WordPress Plugin</a>
          <a href="/articles.html" class="mega-link">CDN Integration</a>
        </div>
        <div>
          <div class="mega-column__label">Protocols</div>
          <a href="/articles.html" class="mega-link">RSL Protocol</a>
          <a href="/articles.html" class="mega-link">llms.txt Specification</a>
          <a href="/articles.html" class="mega-link">robots.txt for AI</a>
          <a href="/articles.html" class="mega-link">Machine-Readable Terms</a>
        </div>
        <div>
          <div class="mega-column__label">Quick Start</div>
          <div style="background: linear-gradient(135deg, rgba(16,185,129,0.06), rgba(79,70,229,0.04)); border: 1px solid rgba(16,185,129,0.2); border-radius: 12px; padding: 1.5rem;">
            <h3 style="font-family: var(--font-display); font-size: 1.125rem; font-weight: 600; color: var(--text-primary); margin-bottom: 0.5rem;">Skip the learning curve.</h3>
            <p style="font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 1rem;">Complete pay-per-crawl implementation. Templates, pricing, contracts.</p>
            <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
          </div>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Pricing -->
    <div id="mega-pricing" class="mega-panel" role="region" aria-label="Pricing models">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Models</div>
          <a href="/articles.html" class="mega-link">Per-Crawl Pricing</a>
          <a href="/articles.html" class="mega-link">Flat-Rate Annual</a>
          <a href="/articles.html" class="mega-link">Tiered Pricing</a>
          <a href="/articles.html" class="mega-link">Volume Discounts</a>
          <a href="/articles.html" class="mega-link">Hybrid Models</a>
        </div>
        <div>
          <div class="mega-column__label">Benchmarks</div>
          <a href="/articles.html" class="mega-link">Rate Cards by Content Type</a>
          <a href="/articles.html" class="mega-link">Revenue Calculators</a>
          <a href="/articles.html" class="mega-link">Industry Benchmarks</a>
          <a href="/articles.html" class="mega-link">Deal Comparisons</a>
        </div>
        <div>
          <div class="mega-column__label">Recent Deals</div>
          <a href="/articles.html" class="mega-link">News Corp &times; OpenAI ($250M)</a>
          <a href="/articles.html" class="mega-link">Reddit &times; Google ($60M/yr)</a>
          <a href="/articles.html" class="mega-link">FT &times; Anthropic</a>
          <a href="/articles.html" class="mega-link">AP &times; OpenAI</a>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Crawlers -->
    <div id="mega-crawlers" class="mega-panel" role="region" aria-label="Crawler information">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Major Crawlers</div>
          <a href="/articles.html" class="mega-link">GPTBot (OpenAI)</a>
          <a href="/articles.html" class="mega-link">ClaudeBot (Anthropic)</a>
          <a href="/articles.html" class="mega-link">Googlebot-Extended</a>
          <a href="/articles.html" class="mega-link">Bytespider (ByteDance)</a>
          <a href="/articles.html" class="mega-link">CCBot (Common Crawl)</a>
        </div>
        <div>
          <div class="mega-column__label">Detection</div>
          <a href="/articles.html" class="mega-link">Bot Detection Methods</a>
          <a href="/articles.html" class="mega-link">User Agent Reference</a>
          <a href="/articles.html" class="mega-link">IP Range Verification</a>
          <a href="/articles.html" class="mega-link">Compliance Rates</a>
        </div>
        <div>
          <div class="mega-column__label">Directory</div>
          <a href="/articles.html" class="mega-link">Full Crawler Directory</a>
          <a href="/articles.html" class="mega-link">Crawler Behavior Matrix</a>
          <a href="/articles.html" class="mega-link">robots.txt Respect Rates</a>
        </div>
      </div>
    </div>

    <!-- Mobile Menu -->
    <div id="mobile-menu" class="mobile-menu">
      <a href="/articles.html" class="mobile-menu__link">Implementation</a>
      <a href="/articles.html" class="mobile-menu__link">Pricing</a>
      <a href="/articles.html" class="mobile-menu__link">Crawlers</a>
      <a href="/articles.html" class="mobile-menu__link">Legal</a>
      <a href="/articles.html" class="mobile-menu__link">All Articles</a>
      <a href="/setup.html" class="mobile-menu__cta">Master the Protocol &mdash; $2,497</a>
    </div>
  </nav>

  <main class="pt-nav">
    <div class="container--narrow" style="padding-top: var(--sp-8);">
      <div class="breadcrumbs">
        <a href="/">Home</a><span class="breadcrumbs__sep">/</span>
        <a href="/articles.html">Articles</a><span class="breadcrumbs__sep">/</span>
        <span>Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide</span>
      </div>

      <header style="margin-bottom: var(--sp-12);">
        <span class="label" style="margin-bottom: var(--sp-4); display: block;">Article &middot; 16 min read</span>
        <h1>Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide</h1>
        <p style="font-size: 1.125rem; color: var(--text-secondary); margin-top: var(--sp-4); max-width: 640px;">Build production-grade AI crawler monitoring infrastructure using Prometheus metrics and Grafana dashboards. Tracks GPTBot, CCBot, ClaudeBot bandwidth, compliance, and anomaly detection.</p>
      </header>

      <article class="article-body">
        <h1>Monitoring AI Crawler Traffic with Prometheus and Grafana: Complete Implementation Guide</h1>
<p><strong>Publishers processing 500M+ AI crawler requests annually lack visibility into crawler behavior patterns, compliance violations, and infrastructure impact.</strong> Traditional web analytics collapse crawler traffic into undifferentiated bot segments, obscuring critical intelligence about which AI companies consume your content, how aggressively they crawl, and whether they honor your robots.txt directives.</p>
<p><strong>Prometheus</strong> and <strong>Grafana</strong> solve this by instrumenting crawler-specific metrics at server level and visualizing them through real-time dashboards. This guide implements production monitoring infrastructure that tracks 14 distinct AI crawler user-agents, measures bandwidth consumption per crawler, detects robots.txt violations, alerts on anomalous traffic spikes, and calculates per-crawler content targeting patterns.</p>
<p>Publishers operating this infrastructure surface data that informs <a href="position-publication-ai-deal.html">AI licensing deal</a> negotiations, provides compliance evidence for contract enforcement, and protects server resources from abusive crawlers.</p>
<h2>Prometheus Fundamentals for Crawler Monitoring</h2>
<p><strong>Prometheus</strong> is an open-source metrics collection and alerting system built for instrumenting modern infrastructure. It scrapes numeric time-series data from configured endpoints, stores it in a time-series database, and exposes query language (PromQL) for analysis.</p>
<p>For AI crawler monitoring, Prometheus collects metrics from your web server access logs or application middleware, aggregates crawler traffic by user-agent, and makes this data queryable for Grafana visualization or alert rule evaluation.</p>
<p><strong>Core architecture components</strong>: Prometheus server (scrapes and stores metrics), exporters (expose metrics from systems like Nginx or Apache), pushgateway (accepts metrics from batch jobs), and alertmanager (routes alerts to notification channels).</p>
<p>The monitoring flow: Your web server logs every HTTP request including user-agent string → Exporter process parses logs and exposes metrics at <code>/metrics</code> endpoint → Prometheus server scrapes this endpoint every 15 seconds → Metrics stored in time-series database → Grafana queries Prometheus to build visualizations.</p>
<p><strong>Metric types for crawler monitoring</strong>: Counters (cumulative totals like total requests per crawler), gauges (current values like active crawler connections), histograms (distribution data like response time percentiles), and summaries (similar to histograms with client-side percentile calculation).</p>
<p>AI crawler monitoring primarily uses <strong>counters</strong> (request counts, bandwidth totals) and <strong>gauges</strong> (requests per second, concurrent crawler sessions). Histograms track response time distributions when crawler behavior impacts server performance.</p>
<p><strong>Label dimensions</strong> segment metrics by relevant attributes. For crawler monitoring, essential labels include: <code>user_agent</code> (specific crawler identifier), <code>crawler_family</code> (OpenAI, Anthropic, Google), <code>http_status</code> (response codes), <code>content_type</code> (HTML, JSON, images), and <code>path_prefix</code> (site sections accessed).</p>
<p>This labeling structure enables queries like &quot;show me GPTBot request rates for HTML content in /articles/ section returning 200 status codes&quot; — precisely the granularity needed to understand crawler content targeting patterns.</p>
<h2>Infrastructure Setup: Prometheus Installation and Configuration</h2>
<p><strong>Deploy Prometheus server</strong> on dedicated infrastructure separate from your web application servers. This isolation prevents monitoring overhead from competing with application resources.</p>
<p>Install Prometheus via Docker for containerized deployments or via system packages for traditional server environments. Docker deployment:</p>
<pre><code class="language-bash">docker run -d \
  --name prometheus \
  -p 9090:9090 \
  -v /opt/prometheus/config:/etc/prometheus \
  -v /opt/prometheus/data:/prometheus \
  prom/prometheus:latest \
  --config.file=/etc/prometheus/prometheus.yml \
  --storage.tsdb.retention.time=90d
</code></pre>
<p>The <code>--storage.tsdb.retention.time=90d</code> flag configures 90-day metric retention, providing sufficient historical data for trend analysis during licensing negotiations while managing disk usage.</p>
<p><strong>Configure scrape targets</strong> in <code>prometheus.yml</code> pointing to your metrics exporters:</p>
<pre><code class="language-yaml">global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: &#39;nginx-ai-crawlers&#39;
    static_configs:
      - targets: [&#39;web01.example.com:9113&#39;, &#39;web02.example.com:9113&#39;]
        labels:
          environment: &#39;production&#39;

  - job_name: &#39;apache-ai-crawlers&#39;
    static_configs:
      - targets: [&#39;web03.example.com:9117&#39;]
        labels:
          environment: &#39;production&#39;
</code></pre>
<p>This configuration scrapes metrics from Nginx exporters on web01/web02 and Apache exporter on web03 every 15 seconds. The <code>job_name</code> labels distinguish metric sources in queries.</p>
<p><strong>Configure recording rules</strong> to pre-compute frequently-queried aggregations. Recording rules calculate complex queries periodically and store results as new metrics, improving dashboard query performance.</p>
<p>Create <code>/etc/prometheus/rules/ai-crawlers.yml</code>:</p>
<pre><code class="language-yaml">groups:
  - name: ai_crawler_aggregations
    interval: 60s
    rules:
      - record: ai_crawler:requests:rate5m
        expr: rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[5m])

      - record: ai_crawler:bandwidth:rate5m
        expr: rate(http_response_bytes_total{user_agent=~&quot;.*Bot.*&quot;}[5m])

      - record: ai_crawler:requests_by_family:rate1h
        expr: sum by (crawler_family) (rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[1h]))
</code></pre>
<p>These rules calculate 5-minute request rates, 5-minute bandwidth rates, and hourly request rates grouped by crawler family (OpenAI, Anthropic, Google). Pre-computing these aggregations reduces Grafana dashboard query latency from 2-3 seconds to under 200ms.</p>
<p>Reference recording rules in <code>prometheus.yml</code>:</p>
<pre><code class="language-yaml">rule_files:
  - &#39;/etc/prometheus/rules/ai-crawlers.yml&#39;
</code></pre>
<p><strong>Configure alert rules</strong> to notify when crawlers violate rate limits or behave anomalously. Create <code>/etc/prometheus/rules/ai-crawler-alerts.yml</code>:</p>
<pre><code class="language-yaml">groups:
  - name: ai_crawler_alerts
    interval: 30s
    rules:
      - alert: ExcessiveCrawlerRate
        expr: rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[5m]) &gt; 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: &quot;AI crawler {{ $labels.user_agent }} exceeds rate limit&quot;
          description: &quot;{{ $labels.user_agent }} averaging {{ $value }} requests/sec for 5+ minutes&quot;

      - alert: RobotsTxtViolation
        expr: increase(http_requests_total{user_agent=~&quot;.*Bot.*&quot;,path=&quot;/restricted/.*&quot;,http_status=&quot;403&quot;}[1h]) &gt; 10
        labels:
          severity: critical
        annotations:
          summary: &quot;Crawler {{ $labels.user_agent }} violating robots.txt&quot;
          description: &quot;{{ $labels.user_agent }} attempted {{ $value }} requests to restricted paths in past hour&quot;
</code></pre>
<p>The <code>ExcessiveCrawlerRate</code> alert fires when any crawler exceeds 100 requests/second for 5+ consecutive minutes. The <code>RobotsTxtViolation</code> alert triggers when crawlers attempt 10+ requests to blocked paths within an hour.</p>
<h2>Nginx Metrics Exporter Configuration</h2>
<p><strong>Nginx metrics exporters</strong> translate web server access logs into Prometheus metrics. Two primary approaches: <strong>nginx-prometheus-exporter</strong> (parses Nginx stub_status module) and <strong>mtail</strong> (parses access logs directly).</p>
<p>For comprehensive AI crawler monitoring, <strong>mtail</strong> provides superior detail because it accesses full request metadata including user-agents, paths, and response sizes. The stub_status approach lacks per-request granularity needed for crawler-specific metrics.</p>
<p>Install <strong>mtail</strong> on each Nginx server:</p>
<pre><code class="language-bash">wget https://github.com/google/mtail/releases/download/v3.0.0-rc53/mtail_v3.0.0-rc53_linux_amd64
chmod +x mtail_v3.0.0-rc53_linux_amd64
mv mtail_v3.0.0-rc53_linux_amd64 /usr/local/bin/mtail
</code></pre>
<p>Create mtail program <code>/etc/mtail/ai-crawlers.mtail</code> to parse Nginx access logs and expose crawler metrics:</p>
<pre><code class="language-mtail">counter http_requests_total by user_agent, http_status, content_type, crawler_family
counter http_response_bytes_total by user_agent, crawler_family

# Define AI crawler families
def get_crawler_family(ua) {
  /GPTBot/ { return &quot;OpenAI&quot; }
  /ClaudeBot/ { return &quot;Anthropic&quot; }
  /CCBot/ { return &quot;CommonCrawl&quot; }
  /Google-Extended/ { return &quot;Google&quot; }
  /anthropic-ai/ { return &quot;Anthropic&quot; }
  /Bytespider/ { return &quot;ByteDance&quot; }
  /Applebot-Extended/ { return &quot;Apple&quot; }
  /FacebookBot/ { return &quot;Meta&quot; }
  /cohere-ai/ { return &quot;Cohere&quot; }
  /PerplexityBot/ { return &quot;Perplexity&quot; }
  /Omgilibot/ { return &quot;Omgili&quot; }
  /YouBot/ { return &quot;You.com&quot; }
  /Diffbot/ { return &quot;Diffbot&quot; }
  /ImagesiftBot/ { return &quot;Imagesift&quot; }
  return &quot;Other&quot;
}

# Parse Nginx combined log format
/^/ +
/(?P&lt;remote_addr&gt;\S+) \S+ \S+ \[(?P&lt;timestamp&gt;[^\]]+)\] / +
/&quot;(?P&lt;method&gt;\S+) (?P&lt;path&gt;\S+) \S+&quot; / +
/(?P&lt;status&gt;\d{3}) (?P&lt;bytes_sent&gt;\d+) / +
/&quot;(?P&lt;referrer&gt;[^&quot;]*)&quot; &quot;(?P&lt;user_agent&gt;[^&quot;]*)&quot;/ {

  $ua = $user_agent
  $family = get_crawler_family($ua)

  # Only track AI crawler requests
  $family != &quot;Other&quot; {
    http_requests_total[$ua][strptime($timestamp, &quot;02/Jan/2006:15:04:05 -0700&quot;)][int($status)][&quot;unknown&quot;][$family]++
    http_response_bytes_total[$ua][strptime($timestamp, &quot;02/Jan/2006:15:04:05 -0700&quot;)][$family] += int($bytes_sent)
  }
}
</code></pre>
<p>This mtail program parses Nginx combined log format, extracts user-agent strings, maps them to crawler families (OpenAI, Anthropic, etc.), and increments request counters and bandwidth totals per crawler.</p>
<p>Launch mtail as systemd service. Create <code>/etc/systemd/system/mtail.service</code>:</p>
<pre><code class="language-ini">[Unit]
Description=mtail log processor
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/bin/mtail \
  --progs /etc/mtail \
  --logs /var/log/nginx/access.log \
  --port 9113
Restart=always
User=mtail

[Install]
WantedBy=multi-user.target
</code></pre>
<p>Enable and start:</p>
<pre><code class="language-bash">useradd -r -s /bin/false mtail
systemctl enable mtail
systemctl start mtail
</code></pre>
<p>Verify metrics exposure:</p>
<pre><code class="language-bash">curl http://localhost:9113/metrics | grep ai_crawler
</code></pre>
<p>You should see output like:</p>
<pre><code>http_requests_total{user_agent=&quot;GPTBot/1.0&quot;,http_status=&quot;200&quot;,crawler_family=&quot;OpenAI&quot;} 1847
http_requests_total{user_agent=&quot;ClaudeBot/1.0&quot;,http_status=&quot;200&quot;,crawler_family=&quot;Anthropic&quot;} 923
http_response_bytes_total{user_agent=&quot;GPTBot/1.0&quot;,crawler_family=&quot;OpenAI&quot;} 8472634
</code></pre>
<p>This confirms mtail successfully parses logs and exposes crawler-specific metrics for Prometheus scraping.</p>
<h2>Apache Metrics Exporter Configuration</h2>
<p><strong>Apache servers</strong> require similar log parsing but use different log format syntax. Deploy mtail with Apache-specific parsing rules.</p>
<p>Apache combined log format differs slightly from Nginx. Create <code>/etc/mtail/apache-ai-crawlers.mtail</code>:</p>
<pre><code class="language-mtail">counter http_requests_total by user_agent, http_status, crawler_family
counter http_response_bytes_total by user_agent, crawler_family

def get_crawler_family(ua) {
  /GPTBot/ { return &quot;OpenAI&quot; }
  /ClaudeBot/ { return &quot;Anthropic&quot; }
  /CCBot/ { return &quot;CommonCrawl&quot; }
  /Google-Extended/ { return &quot;Google&quot; }
  /anthropic-ai/ { return &quot;Anthropic&quot; }
  /Bytespider/ { return &quot;ByteDance&quot; }
  /Applebot-Extended/ { return &quot;Apple&quot; }
  /FacebookBot/ { return &quot;Meta&quot; }
  /cohere-ai/ { return &quot;Cohere&quot; }
  /PerplexityBot/ { return &quot;Perplexity&quot; }
  return &quot;Other&quot;
}

# Apache combined log format
/^(?P&lt;remote_host&gt;\S+) \S+ \S+ \[(?P&lt;timestamp&gt;[^\]]+)\] / +
/&quot;(?P&lt;method&gt;\S+) (?P&lt;url&gt;\S+) \S+&quot; / +
/(?P&lt;status&gt;\d{3}) (?P&lt;bytes_sent&gt;\S+) / +
/&quot;(?P&lt;referrer&gt;[^&quot;]*)&quot; &quot;(?P&lt;user_agent&gt;[^&quot;]*)&quot;/ {

  $ua = $user_agent
  $family = get_crawler_family($ua)

  $family != &quot;Other&quot; {
    http_requests_total[$ua][strptime($timestamp, &quot;02/Jan/2006:15:04:05 -0700&quot;)][int($status)][$family]++

    # Handle Apache&#39;s &quot;-&quot; for zero bytes
    $bytes_sent != &quot;-&quot; {
      http_response_bytes_total[$ua][strptime($timestamp, &quot;02/Jan/2006:15:04:05 -0700&quot;)][$family] += int($bytes_sent)
    }
  }
}
</code></pre>
<p>Configure Apache mtail service pointing to Apache log paths:</p>
<pre><code class="language-ini">[Service]
ExecStart=/usr/local/bin/mtail \
  --progs /etc/mtail \
  --logs /var/log/apache2/access.log \
  --port 9117
</code></pre>
<h2>Grafana Dashboard Construction</h2>
<p><strong>Grafana</strong> transforms Prometheus metrics into visual dashboards. Install Grafana via Docker or system packages:</p>
<pre><code class="language-bash">docker run -d \
  --name grafana \
  -p 3000:3000 \
  -v /opt/grafana/data:/var/lib/grafana \
  grafana/grafana:latest
</code></pre>
<p>Access Grafana at <code>http://localhost:3000</code> (default credentials: admin/admin).</p>
<p><strong>Add Prometheus data source</strong>: Configuration → Data Sources → Add data source → Prometheus. Configure URL pointing to your Prometheus server: <code>http://prometheus-server:9090</code>.</p>
<p><strong>Create AI Crawler Overview dashboard</strong> with six panels:</p>
<h3>Panel 1: Request Rate by Crawler Family (Time Series)</h3>
<p>This panel visualizes request volume trends across all AI crawler families over time, revealing traffic patterns and comparative crawler activity.</p>
<p>Query:</p>
<pre><code class="language-promql">sum by (crawler_family) (rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[5m]))
</code></pre>
<p>Visualization: Time series line chart
Legend: <code>{{ crawler_family }}</code>
Unit: requests/second</p>
<p>This query calculates 5-minute average request rates grouped by crawler family. The output shows separate lines for OpenAI, Anthropic, Google, etc., making traffic distribution immediately visible.</p>
<h3>Panel 2: Bandwidth Consumption by Crawler (Gauge)</h3>
<p>Current bandwidth consumption rates per crawler, useful for identifying resource-intensive crawlers.</p>
<p>Query:</p>
<pre><code class="language-promql">topk(10, sum by (user_agent, crawler_family) (rate(http_response_bytes_total[5m])))
</code></pre>
<p>Visualization: Bar gauge
Unit: bytes/second
Thresholds: Green (0-10MB/s), Yellow (10-50MB/s), Red (&gt;50MB/s)</p>
<p>The <code>topk(10, ...)</code> function returns the 10 highest bandwidth consumers, highlighting crawlers imposing the greatest infrastructure load.</p>
<h3>Panel 3: Total Requests by Status Code (Stat panel)</h3>
<p>Summary statistics showing request success rates and error distributions.</p>
<p>Queries:</p>
<pre><code class="language-promql"># Total successful requests
sum(increase(http_requests_total{http_status=~&quot;2..&quot;}[24h]))

# Total client errors
sum(increase(http_requests_total{http_status=~&quot;4..&quot;}[24h]))

# Total server errors
sum(increase(http_requests_total{http_status=~&quot;5..&quot;}[24h]))
</code></pre>
<p>Create three stat panels displaying 24-hour request totals by status code class. Color code: Green (2xx), Orange (4xx), Red (5xx).</p>
<p>This panel surfaces crawler-induced error rates. High 4xx counts indicate crawlers requesting non-existent content. High 5xx counts suggest crawler traffic overwhelming server capacity.</p>
<h3>Panel 4: Robots.txt Violations (Table)</h3>
<p>Detailed table listing crawlers attempting to access blocked paths, providing compliance monitoring and evidence for contract enforcement.</p>
<p>Query:</p>
<pre><code class="language-promql">topk(20, sum by (user_agent, path, crawler_family) (increase(http_requests_total{path=~&quot;/admin/.*|/api/internal/.*|/restricted/.*&quot;, http_status=&quot;403&quot;}[24h])))
</code></pre>
<p>Visualization: Table
Columns: User Agent, Crawler Family, Path, Violation Count</p>
<p>This query identifies crawlers receiving 403 Forbidden responses on paths typically blocked via robots.txt. The table format makes violations easily reviewable for compliance teams or legal counsel during licensing disputes.</p>
<h3>Panel 5: Content Type Distribution (Pie chart)</h3>
<p>Breakdown of what content types crawlers access most frequently — HTML pages, images, JavaScript files, etc.</p>
<p>Query:</p>
<pre><code class="language-promql">sum by (content_type) (increase(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[24h]))
</code></pre>
<p>Visualization: Pie chart
Legend: <code>{{ content_type }}</code></p>
<p>This reveals crawler content targeting preferences. AI companies primarily training on text content will show 90%+ HTML requests. Those building multimodal models exhibit higher image/video request proportions.</p>
<h3>Panel 6: Anomaly Detection - Request Rate Spikes (Graph with annotations)</h3>
<p>Time series highlighting abnormal traffic spikes that deviate from baseline patterns.</p>
<p>Query:</p>
<pre><code class="language-promql">(
  rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[5m])
  -
  avg_over_time(rate(http_requests_total{user_agent=~&quot;.*Bot.*&quot;}[5m])[1h:5m])
) &gt; 50
</code></pre>
<p>Visualization: Time series
Annotations: Mark points where deviation exceeds 50 requests/second above hourly average</p>
<p>This query calculates the difference between current request rate and 1-hour rolling average. When the difference exceeds 50 req/sec, the dashboard highlights these spikes as anomalies worthy of investigation.</p>
<p>Anomaly spikes indicate either crawler misbehavior (ignoring rate limits) or new crawler discovery phases (first-time comprehensive crawls of your content).</p>
<h2>Advanced Metrics: Per-Crawler Content Targeting Analysis</h2>
<p><strong>Content targeting patterns</strong> reveal which site sections and content types specific crawlers prioritize. This intelligence informs licensing negotiations — crawlers heavily targeting premium content justify higher licensing fees.</p>
<p>Implement path-prefix labeling in your mtail program to track section-level crawler behavior. Extend the mtail parsing logic:</p>
<pre><code class="language-mtail">counter http_requests_by_section by user_agent, crawler_family, section

def get_section(path) {
  /^\/articles\// { return &quot;articles&quot; }
  /^\/guides\// { return &quot;guides&quot; }
  /^\/api-docs\// { return &quot;api-docs&quot; }
  /^\/blog\// { return &quot;blog&quot; }
  /^\/research\// { return &quot;research&quot; }
  return &quot;other&quot;
}

# In your main parsing block, add:
$section = get_section($path)
http_requests_by_section[$ua][$family][$section]++
</code></pre>
<p>Create Grafana heatmap panel showing crawler content targeting:</p>
<p>Query:</p>
<pre><code class="language-promql">sum by (crawler_family, section) (increase(http_requests_by_section[24h]))
</code></pre>
<p>Visualization: Heatmap
X-axis: crawler_family
Y-axis: section
Color intensity: Request volume</p>
<p>This heatmap instantly reveals that OpenAI&#39;s GPTBot focuses 60% of requests on <code>/articles/</code> while Anthropic&#39;s ClaudeBot distributes requests more evenly across <code>/guides/</code> and <code>/research/</code>. These patterns inform content strategy — if <code>/research/</code> attracts disproportionate AI crawler interest, that section provides outsized licensing value.</p>
<h2>Bandwidth Accounting and Cost Attribution</h2>
<p><strong>Bandwidth costs</strong> from aggressive crawlers can reach $500-$5,000 monthly for high-traffic publications. Attributing these costs to specific crawlers justifies <a href="position-publication-ai-deal.html">licensing fee negotiations</a> that offset infrastructure expenses.</p>
<p>Track bandwidth metrics with egress cost calculations. Most cloud providers charge $0.08-$0.12 per GB egress. Configure Grafana to multiply bandwidth metrics by your provider&#39;s rates.</p>
<p>Create calculated metric in Grafana dashboard:</p>
<p>Query:</p>
<pre><code class="language-promql">sum by (crawler_family) (increase(http_response_bytes_total[30d])) * 0.09 / 1e9
</code></pre>
<p>This query calculates total bytes transferred per crawler family over 30 days, converts to gigabytes (÷ 1e9), and multiplies by $0.09/GB egress rate.</p>
<p>Visualization: Table
Columns: Crawler Family, Bandwidth (GB), Estimated Cost ($)</p>
<p>The resulting table quantifies monthly infrastructure costs attributable to each AI company&#39;s crawler. If OpenAI&#39;s GPTBot consumes 800GB generating $72 in egress costs, you possess concrete cost data supporting licensing fee negotiations.</p>
<p>Present this data during deal discussions: &quot;Your crawler imposes $72 monthly in direct infrastructure costs plus server resource consumption. Our minimum licensing fee must offset these operational expenses.&quot;</p>
<h2>Compliance Monitoring: Detecting Robots.txt Violations</h2>
<p><strong>Robots.txt compliance monitoring</strong> detects crawlers ignoring your directives and provides evidence for contract enforcement or legal action.</p>
<p>Configure alerts that trigger when crawlers access blocked paths. Extend Prometheus alert rules:</p>
<pre><code class="language-yaml">- alert: RobotsTxtSystematicViolation
  expr: |
    sum by (user_agent, crawler_family) (
      increase(http_requests_total{
        path=~&quot;/admin/.*|/wp-admin/.*|/api/internal/.*&quot;,
        http_status=&quot;403&quot;
      }[24h])
    ) &gt; 50
  labels:
    severity: critical
  annotations:
    summary: &quot;Crawler {{ $labels.user_agent }} systematically violating robots.txt&quot;
    description: &quot;{{ $labels.user_agent }} from {{ $labels.crawler_family }} attempted {{ $value }} blocked requests in 24h&quot;
</code></pre>
<p>This alert fires when crawlers attempt 50+ requests to blocked paths within 24 hours, indicating systematic (not accidental) violations.</p>
<p>Route these alerts to your legal team or business development contacts managing AI licensing relationships. Systematic violations breach licensing agreements and justify immediate escalation.</p>
<p>Build a compliance evidence dashboard compiling violation history for legal proceedings:</p>
<p><strong>Compliance Report Panel</strong>:
Query:</p>
<pre><code class="language-promql">sum by (user_agent, crawler_family, path) (
  increase(http_requests_total{
    path=~&quot;/admin/.*|/api/internal/.*|/restricted/.*&quot;,
    http_status=&quot;403&quot;
  }[90d])
)
</code></pre>
<p>Visualization: Table with export functionality
Columns: Crawler, Family, Blocked Path, Violation Count, First Seen, Last Seen</p>
<p>Export this table to CSV for inclusion in legal demand letters or licensing renegotiation materials. The data provides timestamped evidence of specific violations with request counts and targeted paths.</p>
<h2>Performance Impact Analysis: Server Load Attribution</h2>
<p><strong>Crawler traffic</strong> can degrade site performance for human visitors. Measuring performance impact per crawler informs rate-limiting decisions and justifies crawler restrictions.</p>
<p>Track response time percentiles by crawler using histogram metrics. Extend mtail program:</p>
<pre><code class="language-mtail">histogram http_request_duration_seconds by user_agent, crawler_family buckets 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0

# In parsing block, extract request duration from Nginx
/^(?P&lt;remote_addr&gt;\S+) \S+ \S+ \[(?P&lt;timestamp&gt;[^\]]+)\] / +
/&quot;(?P&lt;method&gt;\S+) (?P&lt;path&gt;\S+) \S+&quot; / +
/(?P&lt;status&gt;\d{3}) (?P&lt;bytes_sent&gt;\d+) / +
/&quot;(?P&lt;referrer&gt;[^&quot;]*)&quot; &quot;(?P&lt;user_agent&gt;[^&quot;]*)&quot; / +
/(?P&lt;duration&gt;\d+\.\d+)/ {

  $ua = $user_agent
  $family = get_crawler_family($ua)

  $family != &quot;Other&quot; {
    http_request_duration_seconds[$ua][$family] = float($duration)
  }
}
</code></pre>
<p>This requires configuring Nginx to log request duration. Add <code>$request_time</code> to your Nginx log format:</p>
<pre><code class="language-nginx">log_format combined_with_duration &#39;$remote_addr - $remote_user [$time_local] &#39;
                                  &#39;&quot;$request&quot; $status $body_bytes_sent &#39;
                                  &#39;&quot;$http_referer&quot; &quot;$http_user_agent&quot; $request_time&#39;;
</code></pre>
<p>Create Grafana panel comparing response times:</p>
<p>Query:</p>
<pre><code class="language-promql">histogram_quantile(0.95, sum by (crawler_family, le) (rate(http_request_duration_seconds_bucket[5m])))
</code></pre>
<p>Visualization: Time series
Legend: <code>P95 {{ crawler_family }}</code>
Unit: seconds</p>
<p>This displays 95th percentile response times per crawler family. If ClaudeBot&#39;s p95 response time is 0.8 seconds while GPTBot&#39;s is 0.3 seconds, ClaudeBot imposes greater server load — possibly due to requesting more complex pages or overwhelming connection pools.</p>
<p>Correlate crawler traffic volume with overall site performance:</p>
<p>Query:</p>
<pre><code class="language-promql">histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{user_agent!~&quot;.*Bot.*&quot;}[5m])) by (le))
</code></pre>
<p>This calculates p95 response times for human (non-bot) traffic. Overlay this with total crawler request rates to visualize performance degradation during crawler traffic spikes.</p>
<p>If human visitor p95 response time increases from 0.4s to 1.2s during crawler traffic spikes, crawler activity demonstrably harms user experience — justifying aggressive rate limiting or licensing fee increases to offset performance impact.</p>
<h2>Alert Configuration and Incident Response Workflows</h2>
<p><strong>Alertmanager</strong> routes Prometheus alerts to appropriate notification channels and manages alert lifecycle (grouping, inhibition, silencing).</p>
<p>Install Alertmanager:</p>
<pre><code class="language-bash">docker run -d \
  --name alertmanager \
  -p 9093:9093 \
  -v /opt/alertmanager/config:/etc/alertmanager \
  prom/alertmanager:latest
</code></pre>
<p>Configure <code>/opt/alertmanager/config/alertmanager.yml</code>:</p>
<pre><code class="language-yaml">global:
  resolve_timeout: 5m

route:
  group_by: [&#39;alertname&#39;, &#39;crawler_family&#39;]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: &#39;team-infrastructure&#39;

  routes:
    - match:
        severity: critical
      receiver: &#39;team-legal&#39;

    - match:
        alertname: ExcessiveCrawlerRate
      receiver: &#39;team-infrastructure&#39;

    - match:
        alertname: RobotsTxtViolation
      receiver: &#39;team-legal&#39;

receivers:
  - name: &#39;team-infrastructure&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK&#39;
        channel: &#39;#infrastructure-alerts&#39;
        title: &#39;AI Crawler Alert: {{ .GroupLabels.alertname }}&#39;
        text: &#39;{{ range .Alerts }}{{ .Annotations.description }}{{ end }}&#39;

  - name: &#39;team-legal&#39;
    email_configs:
      - to: &#39;legal@example.com&#39;
        from: &#39;alerts@example.com&#39;
        smarthost: &#39;smtp.example.com:587&#39;
        auth_username: &#39;alerts@example.com&#39;
        auth_password: &#39;smtp-password&#39;
        headers:
          Subject: &#39;CRITICAL: AI Crawler Violation - {{ .GroupLabels.crawler_family }}&#39;
</code></pre>
<p>This configuration routes infrastructure alerts (excessive rate) to your operations team via Slack while routing compliance violations to legal team via email.</p>
<p>Update Prometheus configuration to use Alertmanager:</p>
<pre><code class="language-yaml">alerting:
  alertmanagers:
    - static_configs:
        - targets: [&#39;localhost:9093&#39;]
</code></pre>
<p><strong>Incident response workflow</strong> for crawler violations:</p>
<ol>
<li><strong>Alert fires</strong> → Alertmanager sends notification to appropriate team</li>
<li><strong>Team reviews</strong> alert details including crawler identity, violation type, and magnitude</li>
<li><strong>For rate violations</strong>: Implement temporary rate limiting via web server config or WAF rules</li>
<li><strong>For robots.txt violations</strong>: Document evidence, compile violation history from Grafana, escalate to licensing contact at AI company</li>
<li><strong>Persistent violations</strong>: Engage legal counsel, prepare demand letter citing violation evidence</li>
</ol>
<p>Automate initial response for severe violations using Alertmanager webhook receiver triggering rate-limiting scripts. Configure webhook receiver in Alertmanager that POSTs to your infrastructure automation API when critical alerts fire, automatically applying temporary crawler blocks while your team investigates.</p>
<h2>Long-Term Analytics: Trend Analysis for Licensing Negotiations</h2>
<p><strong>Historical trend analysis</strong> over 90+ days reveals crawler behavior evolution and supports licensing negotiation positioning.</p>
<p>Create quarterly review dashboard aggregating key metrics:</p>
<p><strong>Panel: 90-Day Crawler Activity Trends</strong>
Query:</p>
<pre><code class="language-promql">sum by (crawler_family) (increase(http_requests_total[90d]))
</code></pre>
<p>Visualization: Bar chart
Shows total requests per crawler family over past 90 days</p>
<p><strong>Panel: Bandwidth Growth Rate</strong>
Query:</p>
<pre><code class="language-promql">(
  sum by (crawler_family) (increase(http_response_bytes_total[30d] offset 30d))
  /
  sum by (crawler_family) (increase(http_response_bytes_total[30d] offset 60d))
  - 1
) * 100
</code></pre>
<p>Visualization: Gauge
Shows month-over-month bandwidth growth percentage</p>
<p>If OpenAI&#39;s crawler bandwidth grew 85% month-over-month, they&#39;re substantially increasing their content consumption from your site. This growth justifies licensing fee increases during renewal negotiations: &quot;Your crawler bandwidth consumption increased 85% since our initial agreement, requiring infrastructure scaling investments that warrant revised pricing.&quot;</p>
<p><strong>Panel: Content Targeting Evolution</strong>
Query:</p>
<pre><code class="language-promql">sum by (section) (increase(http_requests_by_section{crawler_family=&quot;OpenAI&quot;}[7d]))
</code></pre>
<p>Visualization: Stacked area chart
Shows how crawler content targeting shifts across site sections over time</p>
<p>If GPTBot shifted from 40% articles / 60% documentation to 70% articles / 30% documentation over 12 weeks, OpenAI&#39;s priorities changed. This insight informs selective licensing strategies where you offer tiered pricing based on which content sections get access.</p>
<p>Export these trend dashboards as PDFs for inclusion in licensing negotiation materials. Visual evidence of increasing crawler activity, bandwidth consumption, and content targeting evolution substantiates your negotiating position when requesting fee increases or more favorable terms.</p>
<p>Publishers operating comprehensive Prometheus + Grafana monitoring infrastructure enter licensing negotiations with quantified data about crawler behavior, infrastructure costs, and content value utilization — transforming discussions from abstract speculation into evidence-based commercial negotiations.</p>

      </article>

      <div class="cta-box" style="margin: var(--sp-16) 0;">
        <h3>Your Content Feeds AI. Get Paid for It.</h3>
        <p>Complete pay-per-crawl implementation. Templates, pricing models, licensing contracts. Everything.</p>
        <a href="/setup.html" class="btn btn--primary btn--large">Master the Protocol &mdash; $2,497</a>
      </div>

      <div style="margin-top: var(--sp-8); padding-top: var(--sp-8); border-top: 1px solid var(--border);">
        <a href="/articles.html" style="font-family: var(--font-mono); font-size: 0.875rem; font-weight: 500; color: var(--accent);">&larr; All Articles</a>
      </div>
    </div>
  </main>


  <footer class="footer">
    <div class="container">
      <div class="footer__grid">
        <div class="footer__brand">
          <div class="footer__logo">AI Pay Per Crawl</div>
          <p class="footer__desc">Your content feeds AI. The protocol ensures you get paid for it. Implementation guides, pricing models, and licensing infrastructure for publishers monetizing AI crawler traffic.</p>
          <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
        </div>

        <div>
          <div class="footer__heading">Implementation</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Cloudflare Setup</a></li>
            <li><a href="/articles.html">RSL Protocol</a></li>
            <li><a href="/articles.html">llms.txt Guide</a></li>
            <li><a href="/articles.html">Nginx Rules</a></li>
            <li><a href="/articles.html">Apache Config</a></li>
            <li><a href="/articles.html">WordPress Plugin</a></li>
            <li><a href="/articles.html">CDN Integration</a></li>
            <li><a href="/articles.html">robots.txt Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">Pricing &amp; Tools</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Pricing Models</a></li>
            <li><a href="/articles.html">Rate Cards</a></li>
            <li><a href="/articles.html">Revenue Calculator</a></li>
            <li><a href="/articles.html">Crawler Directory</a></li>
            <li><a href="/articles.html">Bot Detection</a></li>
            <li><a href="/articles.html">Compliance Rates</a></li>
            <li><a href="/articles.html">Deal Analysis</a></li>
            <li><a href="/articles.html">Licensing Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">From Scale With Search</div>
          <ul class="footer__links">
            <li><a href="https://scalewithsearch.com" target="_blank" rel="me">Scale With Search</a></li>
            <li><a href="https://aifirstsearch.com" target="_blank" rel="me">AI First Search</a></li>
            <li><a href="https://browserprompt.com" target="_blank" rel="me">Browser Prompt</a></li>
            <li><a href="https://polytraffic.com" target="_blank" rel="me">Polytraffic</a></li>
            <li><a href="https://creatinepedia.com" target="_blank" rel="me">Creatinepedia</a></li>
            <li><a href="https://victorvalentineromo.com" target="_blank" rel="me">Victor Romo</a></li>
            <li><a href="https://b2bvic.com" target="_blank" rel="me">B2B Vic</a></li>
          </ul>
        </div>
      </div>

      <div class="footer__bar">
        <p>&copy; 2026 AI Pay Per Crawl. Built by <a href="https://victorvalentineromo.com" target="_blank">Victor Valentine Romo</a>.</p>
        <p>A <a href="https://scalewithsearch.com" target="_blank">Scale With Search</a> property.</p>
      </div>
    </div>
  </footer>


  <script>
    /* Mega menu */
    (function() {
      var triggers = document.querySelectorAll('.mega-trigger');
      var panels = document.querySelectorAll('.mega-panel');
      var closeTimeout = null;

      function openPanel(id) {
        clearTimeout(closeTimeout);
        panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
        triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        var panel = document.getElementById('mega-' + id);
        var trigger = document.querySelector('[data-mega="' + id + '"] .mega-trigger');
        if (panel) panel.classList.add('mega-panel--open');
        if (trigger) trigger.setAttribute('aria-expanded', 'true');
      }

      function scheduleClose() {
        closeTimeout = setTimeout(function() {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }, 200);
      }

      function cancelClose() { clearTimeout(closeTimeout); }

      triggers.forEach(function(btn) {
        var wrapper = btn.closest('[data-mega]');
        var id = wrapper.getAttribute('data-mega');
        btn.addEventListener('mouseenter', function() { openPanel(id); });
        btn.addEventListener('click', function(e) {
          e.preventDefault();
          var panel = document.getElementById('mega-' + id);
          if (panel && panel.classList.contains('mega-panel--open')) {
            panel.classList.remove('mega-panel--open');
            btn.setAttribute('aria-expanded', 'false');
          } else {
            openPanel(id);
          }
        });
        wrapper.addEventListener('mouseleave', scheduleClose);
      });

      panels.forEach(function(panel) {
        panel.addEventListener('mouseenter', cancelClose);
        panel.addEventListener('mouseleave', scheduleClose);
      });

      document.addEventListener('click', function(e) {
        if (!e.target.closest('[data-mega]') && !e.target.closest('.mega-panel')) {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });

      document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape') {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });
    })();

    /* Mobile menu */
    var mobileMenuBtn = document.getElementById('mobile-menu-btn');
    var mobileMenu = document.getElementById('mobile-menu');
    var menuIcon = document.getElementById('menu-icon');
    var closeIcon = document.getElementById('close-icon');
    if (mobileMenuBtn) {
      mobileMenuBtn.addEventListener('click', function() {
        mobileMenu.classList.toggle('mobile-menu--open');
        menuIcon.classList.toggle('hidden');
        closeIcon.classList.toggle('hidden');
      });
    }

    /* Self-link neutralizer */
    (function() {
      var currentPath = window.location.pathname.replace(/\.html$/, '').replace(/\/$/, '') || '/';
      document.querySelectorAll('a[href]').forEach(function(a) {
        var href = a.getAttribute('href').replace(/\.html$/, '').replace(/\/$/, '') || '/';
        if (href === currentPath) {
          a.removeAttribute('href');
          a.setAttribute('aria-current', 'page');
          a.style.opacity = '0.5';
          a.style.pointerEvents = 'none';
        }
      });
    })();

    /* Copy code button */
    function copyCode(button) {
      var codeBlock = button.closest('.code-block');
      var code = codeBlock.querySelector('code').innerText;
      navigator.clipboard.writeText(code).then(function() {
        button.textContent = 'Copied!';
        setTimeout(function() { button.textContent = 'Copy'; }, 2000);
      });
    }

    /* FAQ Accordion */
    document.querySelectorAll('.accordion__trigger').forEach(function(trigger) {
      trigger.addEventListener('click', function() {
        var expanded = this.getAttribute('aria-expanded') === 'true';
        var body = this.nextElementSibling;
        this.setAttribute('aria-expanded', !expanded);
        body.classList.toggle('accordion__body--open');
      });
    });
  </script>
</body>
</html>