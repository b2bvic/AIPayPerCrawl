<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis | AI Pay Per Crawl</title>
    <meta name="description" content="Comprehensive detection framework for AI crawlers. Identify bots through user agent analysis, IP verification, behavioral patterns, and honeypot traps.">
    <meta name="author" content="Victor Valentine Romo">
    <meta name="robots" content="index, follow">
    <meta property="og:title" content="AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis">
    <meta property="og:description" content="Comprehensive detection framework for AI crawlers. Identify bots through user agent analysis, IP verification, behavioral patterns, and honeypot traps.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://aipaypercrawl.com/articles/ai-crawler-detection-methods">
    <meta property="og:site_name" content="AI Pay Per Crawl">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis">
    <meta name="twitter:description" content="Comprehensive detection framework for AI crawlers. Identify bots through user agent analysis, IP verification, behavioral patterns, and honeypot traps.">
    <link rel="canonical" href="https://aipaypercrawl.com/articles/ai-crawler-detection-methods">
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "TechArticle",
  "headline": "AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis",
  "description": "Comprehensive detection framework for AI crawlers. Identify bots through user agent analysis, IP verification, behavioral patterns, and honeypot traps.",
  "author": {
    "@type": "Person",
    "@id": "https://victorvalentineromo.com/#person",
    "name": "Victor Valentine Romo",
    "url": "https://victorvalentineromo.com"
  },
  "publisher": {
    "@type": "Organization",
    "@id": "https://aipaypercrawl.com/#organization",
    "name": "AI Pay Per Crawl",
    "url": "https://aipaypercrawl.com"
  },
  "datePublished": "2026-02-07",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aipaypercrawl.com/articles/ai-crawler-detection-methods"
  }
}
    </script>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://aipaypercrawl.com"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Articles",
      "item": "https://aipaypercrawl.com/articles.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis",
      "item": "https://aipaypercrawl.com/articles/ai-crawler-detection-methods"
    }
  ]
}
    </script>

    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect fill='%234f46e5' width='100' height='100' rx='12'/><text x='50' y='70' font-family='monospace' font-size='38' font-weight='700' fill='%23ffffff' text-anchor='middle'>APC</text></svg>">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet"></noscript>
    <link rel="stylesheet" href="/base.css">
    <link rel="me" href="https://scalewithsearch.com">
    <link rel="me" href="https://victorvalentineromo.com">
    <link rel="me" href="https://aifirstsearch.com">
    <link rel="me" href="https://browserprompt.com">
    <link rel="me" href="https://creatinepedia.com">
    <link rel="me" href="https://polytraffic.com">
    <link rel="me" href="https://tattooremovalnear.com">
    <link rel="me" href="https://comicstripai.com">
    <link rel="me" href="https://aipaypercrawl.com">
    <link rel="me" href="https://b2bvic.com">
    <link rel="me" href="https://seobyrole.com">
    <link rel="me" href="https://quickfixseo.com">
</head>
<body>


  <nav class="nav" role="navigation" aria-label="Primary">
    <div class="nav__inner">
      <a href="/" class="nav__logo">APC</a>

      <div class="nav__links">
        <!-- Implementation dropdown -->
        <div class="mega-wrapper" data-mega="implementation">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-implementation">
            Implementation
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Pricing dropdown -->
        <div class="mega-wrapper" data-mega="pricing">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-pricing">
            Pricing
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Crawlers dropdown -->
        <div class="mega-wrapper" data-mega="crawlers">
          <button class="nav__link mega-trigger" aria-expanded="false" aria-controls="mega-crawlers">
            Crawlers
            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>
          </button>
        </div>

        <!-- Legal (direct link) -->
        <a href="/articles.html" class="nav__link">Legal</a>

        <div class="nav__divider"></div>

        <a href="/setup.html" class="nav__cta">Master the Protocol &mdash; $2,497</a>
      </div>

      <button class="nav__mobile-btn" id="mobile-menu-btn" aria-label="Open menu">
        <svg id="menu-icon" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
        <svg id="close-icon" class="hidden" width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg>
      </button>
    </div>

    <!-- Mega Panel: Implementation -->
    <div id="mega-implementation" class="mega-panel" role="region" aria-label="Implementation guides">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Infrastructure</div>
          <a href="/articles.html" class="mega-link">Cloudflare Setup</a>
          <a href="/articles.html" class="mega-link">Nginx Rules</a>
          <a href="/articles.html" class="mega-link">Apache Config</a>
          <a href="/articles.html" class="mega-link">WordPress Plugin</a>
          <a href="/articles.html" class="mega-link">CDN Integration</a>
        </div>
        <div>
          <div class="mega-column__label">Protocols</div>
          <a href="/articles.html" class="mega-link">RSL Protocol</a>
          <a href="/articles.html" class="mega-link">llms.txt Specification</a>
          <a href="/articles.html" class="mega-link">robots.txt for AI</a>
          <a href="/articles.html" class="mega-link">Machine-Readable Terms</a>
        </div>
        <div>
          <div class="mega-column__label">Quick Start</div>
          <div style="background: linear-gradient(135deg, rgba(16,185,129,0.06), rgba(79,70,229,0.04)); border: 1px solid rgba(16,185,129,0.2); border-radius: 12px; padding: 1.5rem;">
            <h3 style="font-family: var(--font-display); font-size: 1.125rem; font-weight: 600; color: var(--text-primary); margin-bottom: 0.5rem;">Skip the learning curve.</h3>
            <p style="font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 1rem;">Complete pay-per-crawl implementation. Templates, pricing, contracts.</p>
            <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
          </div>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Pricing -->
    <div id="mega-pricing" class="mega-panel" role="region" aria-label="Pricing models">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Models</div>
          <a href="/articles.html" class="mega-link">Per-Crawl Pricing</a>
          <a href="/articles.html" class="mega-link">Flat-Rate Annual</a>
          <a href="/articles.html" class="mega-link">Tiered Pricing</a>
          <a href="/articles.html" class="mega-link">Volume Discounts</a>
          <a href="/articles.html" class="mega-link">Hybrid Models</a>
        </div>
        <div>
          <div class="mega-column__label">Benchmarks</div>
          <a href="/articles.html" class="mega-link">Rate Cards by Content Type</a>
          <a href="/articles.html" class="mega-link">Revenue Calculators</a>
          <a href="/articles.html" class="mega-link">Industry Benchmarks</a>
          <a href="/articles.html" class="mega-link">Deal Comparisons</a>
        </div>
        <div>
          <div class="mega-column__label">Recent Deals</div>
          <a href="/articles.html" class="mega-link">News Corp &times; OpenAI ($250M)</a>
          <a href="/articles.html" class="mega-link">Reddit &times; Google ($60M/yr)</a>
          <a href="/articles.html" class="mega-link">FT &times; Anthropic</a>
          <a href="/articles.html" class="mega-link">AP &times; OpenAI</a>
        </div>
      </div>
    </div>

    <!-- Mega Panel: Crawlers -->
    <div id="mega-crawlers" class="mega-panel" role="region" aria-label="Crawler information">
      <div class="mega-panel__inner">
        <div>
          <div class="mega-column__label">Major Crawlers</div>
          <a href="/articles.html" class="mega-link">GPTBot (OpenAI)</a>
          <a href="/articles.html" class="mega-link">ClaudeBot (Anthropic)</a>
          <a href="/articles.html" class="mega-link">Googlebot-Extended</a>
          <a href="/articles.html" class="mega-link">Bytespider (ByteDance)</a>
          <a href="/articles.html" class="mega-link">CCBot (Common Crawl)</a>
        </div>
        <div>
          <div class="mega-column__label">Detection</div>
          <a href="/articles.html" class="mega-link">Bot Detection Methods</a>
          <a href="/articles.html" class="mega-link">User Agent Reference</a>
          <a href="/articles.html" class="mega-link">IP Range Verification</a>
          <a href="/articles.html" class="mega-link">Compliance Rates</a>
        </div>
        <div>
          <div class="mega-column__label">Directory</div>
          <a href="/articles.html" class="mega-link">Full Crawler Directory</a>
          <a href="/articles.html" class="mega-link">Crawler Behavior Matrix</a>
          <a href="/articles.html" class="mega-link">robots.txt Respect Rates</a>
        </div>
      </div>
    </div>

    <!-- Mobile Menu -->
    <div id="mobile-menu" class="mobile-menu">
      <a href="/articles.html" class="mobile-menu__link">Implementation</a>
      <a href="/articles.html" class="mobile-menu__link">Pricing</a>
      <a href="/articles.html" class="mobile-menu__link">Crawlers</a>
      <a href="/articles.html" class="mobile-menu__link">Legal</a>
      <a href="/articles.html" class="mobile-menu__link">All Articles</a>
      <a href="/setup.html" class="mobile-menu__cta">Master the Protocol &mdash; $2,497</a>
    </div>
  </nav>

  <main class="pt-nav">
    <div class="container--narrow" style="padding-top: var(--sp-8);">
      <div class="breadcrumbs">
        <a href="/">Home</a><span class="breadcrumbs__sep">/</span>
        <a href="/articles.html">Articles</a><span class="breadcrumbs__sep">/</span>
        <span>AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis</span>
      </div>

      <header style="margin-bottom: var(--sp-12);">
        <span class="label" style="margin-bottom: var(--sp-4); display: block;">Article &middot; 13 min read</span>
        <h1>AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis</h1>
        <p style="font-size: 1.125rem; color: var(--text-secondary); margin-top: var(--sp-4); max-width: 640px;">Comprehensive detection framework for AI crawlers. Identify bots through user agent analysis, IP verification, behavioral patterns, and honeypot traps.</p>
      </header>

      <article class="article-body">
        <h1>AI Crawler Detection Methods: User Agents, IPs, and Behavioral Analysis</h1>
<p>AI crawlers announce themselves. <strong>GPTBot/1.0</strong> in the user agent string. <strong>ClaudeBot/1.0</strong> identifying clearly. <strong>PerplexityBot/1.0</strong> declaring its purpose. Detection seems trivial—grep the logs for known bot names, done.</p>
<p>Until you encounter the ones that don&#39;t identify. User agent spoofing. Residential proxies masking origin. Crawlers deliberately evading detection to bypass robots.txt restrictions. The hidden scraping layer operating parallel to polite, identifiable bots.</p>
<p>Publishers relying solely on user agent strings miss <strong>30-50% of actual AI scraping activity</strong>. The sophisticated scrapers—those violating license terms, ignoring access controls, harvesting paywalled content—don&#39;t advertise their presence.</p>
<p><strong>Detection requires layered approach:</strong> User agent parsing (catches polite bots), IP verification (exposes spoofing), behavioral analysis (identifies bot-like patterns), honeypot traps (surfaces hidden crawlers), fingerprinting (detects automation).</p>
<p>This guide builds comprehensive detection infrastructure combining all methods into unified monitoring system that surfaces both declared and covert AI scraping activity.</p>
<h2>User Agent Detection</h2>
<h3>Known AI Crawler User Agent Database</h3>
<p><strong>Primary detection layer:</strong> Match user agent strings against known AI crawler patterns.</p>
<p><strong>Comprehensive crawler list:</strong></p>
<pre><code>GPTBot
ChatGPT-User
ClaudeBot
Claude-Web
anthropic-ai
PerplexityBot
Perplexity
Google-Extended
GoogleOther
Amazonbot
CCBot
cohere-ai
Omgilibot
Omgili
FacebookBot
Diffbot
ImagesiftBot
img2dataset
Bytespider
YouBot
Applebot-Extended
</code></pre>
<p><strong>Detection script:</strong></p>
<pre><code class="language-bash">#!/bin/bash
AI_PATTERNS=&quot;GPTBot|ChatGPT|ClaudeBot|anthropic-ai|PerplexityBot|Google-Extended|CCBot|Amazonbot|cohere-ai|FacebookBot|Bytespider|YouBot|Applebot-Extended&quot;

grep -E &quot;$AI_PATTERNS&quot; /var/log/nginx/access.log | \
awk -F&#39;&quot;&#39; &#39;{print $1, $6}&#39; | \
awk &#39;{print &quot;IP: &quot;$1&quot; | Bot: &quot;$NF}&#39;
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>IP: 93.184.216.34 | Bot: GPTBot/1.0
IP: 104.28.1.5 | Bot: ClaudeBot/1.0
IP: 142.250.80.14 | Bot: Google-Extended/1.0
</code></pre>
<p><strong>Maintain updated database:</strong> AI companies launch new crawlers. Subscribe to industry resources:</p>
<ul>
<li><a href="ai-crawler-directory-2026.html">ai-crawler-directory-2026.html</a> (comprehensive directory with update frequency)</li>
<li>Bot developer documentation (OpenAI, Anthropic, Google publish crawler specs)</li>
<li>Community-maintained lists (GitHub repositories tracking AI bots)</li>
</ul>
<p><strong>Automation:</strong> Weekly cron job fetches updated bot list, refreshes detection patterns.</p>
<h3>User Agent Parsing and Validation</h3>
<p><strong>Not all user agents claiming to be AI bots are legitimate.</strong></p>
<p><strong>Validation checks:</strong></p>
<p><strong>1. Format consistency</strong></p>
<p>Legitimate bots follow standard user agent format:</p>
<pre><code>BotName/Version (optional details)
</code></pre>
<p><strong>Examples:</strong></p>
<ul>
<li><code>GPTBot/1.0</code> ✓</li>
<li><code>ClaudeBot/1.0 (+https://www.anthropic.com/claude-bot)</code> ✓</li>
<li><code>I am GPTBot please let me scrape</code> ✗ (malformed)</li>
</ul>
<p><strong>Regex validation:</strong></p>
<pre><code class="language-python">import re

def is_valid_bot_format(user_agent):
    pattern = r&#39;^[A-Za-z0-9\-]+/\d+\.\d+(\s|\+|$)&#39;
    return bool(re.match(pattern, user_agent))

# Test
print(is_valid_bot_format(&quot;GPTBot/1.0&quot;))  # True
print(is_valid_bot_format(&quot;Fake Bot&quot;))     # False
</code></pre>
<p><strong>2. Version number plausibility</strong></p>
<p>Bots have version numbers. <strong>GPTBot</strong> started at 1.0. If logs show <code>GPTBot/0.1</code> or <code>GPTBot/99.0</code>, suspicious.</p>
<p><strong>Track known versions:</strong></p>
<pre><code class="language-python">KNOWN_VERSIONS = {
    &#39;GPTBot&#39;: [&#39;1.0&#39;, &#39;1.1&#39;],
    &#39;ClaudeBot&#39;: [&#39;1.0&#39;],
    &#39;PerplexityBot&#39;: [&#39;1.0&#39;, &#39;1.1&#39;]
}

def is_known_version(bot_name, version):
    return version in KNOWN_VERSIONS.get(bot_name, [])
</code></pre>
<p><strong>3. Case sensitivity</strong></p>
<p>Legitimate bots use consistent capitalization. <code>GPTBot</code> not <code>gptbot</code> or <code>GPTBOT</code>.</p>
<p><strong>Spoofing often gets case wrong:</strong></p>
<pre><code class="language-python">def check_case_sensitivity(user_agent):
    known_bots = [&#39;GPTBot&#39;, &#39;ClaudeBot&#39;, &#39;PerplexityBot&#39;]
    for bot in known_bots:
        if bot.lower() in user_agent.lower() and bot not in user_agent:
            return False  # Case mismatch = likely spoof
    return True
</code></pre>
<h3>Detecting User Agent Spoofing</h3>
<p><strong>Spoofing indicators:</strong></p>
<p><strong>Indicator 1: Incomplete user agent</strong></p>
<p>Real bots include details. Spoofed bots often minimal.</p>
<p><strong>Real:</strong> <code>GPTBot/1.0 (+https://openai.com/gptbot)</code></p>
<p><strong>Spoofed:</strong> <code>GPTBot</code></p>
<p><strong>Detection:</strong></p>
<pre><code class="language-python">def is_complete_user_agent(user_agent):
    if &#39;GPTBot&#39; in user_agent:
        return &#39;+https://openai.com&#39; in user_agent or &#39;(+&#39; in user_agent
    return True  # Default pass for unknown bots
</code></pre>
<p><strong>Indicator 2: User agent + browser headers mismatch</strong></p>
<p>Bots send minimal HTTP headers. Browsers send dozens.</p>
<p><strong>Bot headers:</strong></p>
<pre><code>User-Agent: GPTBot/1.0
Accept: */*
Connection: close
</code></pre>
<p><strong>Browser headers:</strong></p>
<pre><code>User-Agent: Mozilla/5.0...
Accept: text/html,application/xhtml+xml,...
Accept-Language: en-US,en;q=0.9
Accept-Encoding: gzip, deflate, br
Cookie: session=abc123...
Referer: https://google.com
... (20+ more headers)
</code></pre>
<p><strong>Spoofed bot (pretending to be GPTBot but actually browser):</strong></p>
<pre><code>User-Agent: GPTBot/1.0
Accept: text/html,application/xhtml+xml,...
Accept-Language: en-US,en;q=0.9
Cookie: ...
</code></pre>
<p><strong>Detection logic:</strong></p>
<p>If user agent claims to be bot BUT includes cookies/referer/accept-language → spoof.</p>
<p><strong>Nginx log format (extended to capture more headers):</strong></p>
<pre><code class="language-nginx">log_format extended &#39;$remote_addr - $remote_user [$time_local] &#39;
                    &#39;&quot;$request&quot; $status $body_bytes_sent &#39;
                    &#39;&quot;$http_referer&quot; &quot;$http_user_agent&quot; &#39;
                    &#39;&quot;$http_accept&quot; &quot;$http_cookie&quot;&#39;;
</code></pre>
<p><strong>Analysis script:</strong></p>
<pre><code class="language-python">def detect_header_mismatch(user_agent, referer, cookie):
    is_bot = any(bot in user_agent for bot in [&#39;GPTBot&#39;, &#39;ClaudeBot&#39;, &#39;PerplexityBot&#39;])
    has_browser_headers = referer or cookie

    if is_bot and has_browser_headers:
        return True  # Spoofed
    return False
</code></pre>
<h2>IP Verification Methods</h2>
<h3>Published IP Range Validation</h3>
<p><strong>AI companies publish official IP ranges.</strong> Verify requests claiming to be from specific bots originate from correct IPs.</p>
<p><strong>OpenAI GPTBot IP ranges (example):</strong></p>
<pre><code>20.163.0.0/16
40.84.180.0/22
52.156.0.0/14
</code></pre>
<p>See <a href="ai-crawler-ip-verification.html">ai-crawler-ip-verification.html</a> for complete range lists and verification methods.</p>
<p><strong>Validation script:</strong></p>
<pre><code class="language-python">import ipaddress

GPTBOT_RANGES = [
    &#39;20.163.0.0/16&#39;,
    &#39;40.84.180.0/22&#39;,
    &#39;52.156.0.0/14&#39;,
]

def is_legitimate_gptbot(ip):
    ip_obj = ipaddress.ip_address(ip)
    for range_str in GPTBOT_RANGES:
        if ip_obj in ipaddress.ip_network(range_str):
            return True
    return False

# Usage
if user_agent == &#39;GPTBot/1.0&#39;:
    if not is_legitimate_gptbot(request_ip):
        log_spoofing_attempt(request_ip, user_agent)
        block_ip(request_ip)
</code></pre>
<p><strong>Cloudflare implementation (firewall rule):</strong></p>
<pre><code>(http.user_agent contains &quot;GPTBot&quot; and not ip.src in {20.163.0.0/16 40.84.180.0/22 52.156.0.0/14})
</code></pre>
<p><strong>Action:</strong> Block or challenge.</p>
<h3>DNS Reverse Lookup Verification</h3>
<p><strong>Alternative to IP range checking:</strong> Verify IP resolves to expected domain.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Extract IP from request</li>
<li>Perform reverse DNS lookup</li>
<li>Check if domain matches bot owner</li>
</ol>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Reverse lookup
host 34.216.144.5

# Output
5.144.216.34.in-addr.arpa domain name pointer crawl-34-216-144-5.ptr.openai.com.
</code></pre>
<p><strong>Domain <code>openai.com</code> confirms legitimate GPTBot.</strong></p>
<p><strong>Spoofed IP would resolve to:</strong></p>
<pre><code>5.0.2.192.in-addr.arpa domain name pointer malicious-server.example.com.
</code></pre>
<p><strong>Automation:</strong></p>
<pre><code class="language-python">import socket

def verify_bot_by_dns(ip, expected_domain):
    try:
        hostname = socket.gethostbyaddr(ip)[0]
        return expected_domain in hostname
    except socket.herror:
        return False  # No reverse DNS = suspicious

# Usage
if user_agent == &#39;ClaudeBot/1.0&#39;:
    if not verify_bot_by_dns(request_ip, &#39;anthropic.com&#39;):
        alert_spoofing()
</code></pre>
<p>See <a href="verify-claudebot-ip-dns.html">verify-claudebot-ip-dns.html</a> for ClaudeBot-specific verification.</p>
<h3>Geographic Origin Analysis</h3>
<p><strong>Most AI crawlers operate from specific regions.</strong></p>
<p><strong>OpenAI GPTBot:</strong> U.S. (Azure data centers)</p>
<p><strong>Anthropic ClaudeBot:</strong> U.S. (AWS us-east/us-west)</p>
<p><strong>Perplexity:</strong> U.S.</p>
<p><strong>Suspicious pattern:</strong> Bot claiming to be GPTBot but originating from China, Russia, or residential ISP in random country.</p>
<p><strong>GeoIP detection:</strong></p>
<pre><code class="language-python">import geoip2.database

reader = geoip2.database.Reader(&#39;/path/to/GeoLite2-City.mmdb&#39;)

def check_geographic_consistency(ip, bot_name):
    response = reader.city(ip)
    country = response.country.iso_code

    expected_countries = {
        &#39;GPTBot&#39;: [&#39;US&#39;],
        &#39;ClaudeBot&#39;: [&#39;US&#39;],
        &#39;PerplexityBot&#39;: [&#39;US&#39;]
    }

    if bot_name in expected_countries:
        return country in expected_countries[bot_name]
    return True  # Unknown bot, pass

if not check_geographic_consistency(request_ip, &#39;GPTBot&#39;):
    log_geographic_anomaly()
</code></pre>
<p><strong>Cloudflare firewall rule:</strong></p>
<pre><code>(http.user_agent contains &quot;GPTBot&quot; and ip.geoip.country ne &quot;US&quot;)
</code></pre>
<p><strong>Action:</strong> Challenge with CAPTCHA.</p>
<h2>Behavioral Analysis</h2>
<h3>Request Pattern Recognition</h3>
<p><strong>Bots exhibit behavioral patterns distinct from humans.</strong></p>
<p><strong>Pattern 1: Sequential URL access</strong></p>
<p>Humans navigate non-linearly (click links, search, jump around).</p>
<p>Bots scrape sequentially (<code>/article/1</code>, <code>/article/2</code>, <code>/article/3</code>...).</p>
<p><strong>Detection:</strong></p>
<pre><code class="language-python">def detect_sequential_scraping(urls, timestamps):
    # Check if URLs follow numeric sequence
    url_ids = [extract_id(url) for url in urls]  # e.g., /article/123 → 123

    sequential = all(url_ids[i] == url_ids[i-1] + 1 for i in range(1, len(url_ids)))

    # Check if requests are rapid
    intervals = [(timestamps[i] - timestamps[i-1]).seconds for i in range(1, len(timestamps))]
    avg_interval = sum(intervals) / len(intervals)

    if sequential and avg_interval &lt; 2:
        return True  # Bot-like behavior
    return False
</code></pre>
<p><strong>Pattern 2: No referrer</strong></p>
<p>Humans arrive via search engines, social media, direct links (have referrer header).</p>
<p>Bots directly request URLs (no referrer or generic referrer).</p>
<p><strong>Analysis:</strong></p>
<pre><code class="language-bash"># Count requests by referrer
awk -F&#39;&quot;&#39; &#39;{print $4}&#39; /var/log/nginx/access.log | sort | uniq -c

# Bots typically show:
# 15000 &quot;-&quot;  (no referrer)
</code></pre>
<p><strong>Detection rule:</strong></p>
<p>If &gt;90% of traffic from single IP has no referrer → likely bot.</p>
<p><strong>Pattern 3: Uniform request intervals</strong></p>
<p>Humans pause (read article, get coffee, browse other sites).</p>
<p>Bots maintain constant request rate (1 request every 2.5 seconds exactly).</p>
<p><strong>Standard deviation analysis:</strong></p>
<pre><code class="language-python">import statistics

def detect_uniform_intervals(timestamps):
    intervals = [(timestamps[i] - timestamps[i-1]).total_seconds() for i in range(1, len(timestamps))]

    if len(intervals) &lt; 10:
        return False  # Need sufficient data

    stddev = statistics.stdev(intervals)
    mean = statistics.mean(intervals)

    # Low stddev relative to mean = uniform timing
    if stddev / mean &lt; 0.1:  # Coefficient of variation &lt; 0.1
        return True  # Bot-like
    return False
</code></pre>
<p><strong>Pattern 4: No JavaScript execution</strong></p>
<p>Browsers execute JavaScript (analytics tracking fires, ads load).</p>
<p>Bots ignore JavaScript (headless crawling).</p>
<p><strong>Detection (JavaScript beacon):</strong></p>
<pre><code class="language-html">&lt;script&gt;
  fetch(&#39;/js-beacon?page=&#39; + location.pathname);
&lt;/script&gt;
</code></pre>
<p><strong>Analysis:</strong></p>
<pre><code class="language-bash"># Compare page views to beacon fires
PAGE_VIEWS=$(grep &quot;/article/&quot; access.log | wc -l)
JS_BEACONS=$(grep &quot;/js-beacon&quot; access.log | wc -l)

BEACON_RATE=$((JS_BEACONS * 100 / PAGE_VIEWS))

# Humans: 80-95% beacon rate
# Bots: 0-10% beacon rate
</code></pre>
<p><strong>Per-IP analysis:</strong></p>
<p>If IP requests 100 pages but 0 beacons → bot.</p>
<h3>Session Duration and Depth Analysis</h3>
<p><strong>Human sessions:</strong> 3-5 minutes, 2-4 pages/visit.</p>
<p><strong>Bot sessions:</strong> Seconds to hours, 50-5000+ pages/visit.</p>
<p><strong>Metric:</strong> Pages per session, session duration.</p>
<p><strong>Calculation:</strong></p>
<pre><code class="language-python">def analyze_session_depth(ip_requests):
    # Group requests by IP, calculate pages/session
    sessions = {}
    for req in ip_requests:
        ip = req[&#39;ip&#39;]
        sessions.setdefault(ip, []).append(req)

    for ip, reqs in sessions.items():
        page_count = len(reqs)
        duration = (reqs[-1][&#39;timestamp&#39;] - reqs[0][&#39;timestamp&#39;]).total_seconds()

        if page_count &gt; 50:  # Threshold
            print(f&quot;Bot candidate: {ip} - {page_count} pages in {duration}s&quot;)
</code></pre>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>&gt;20 pages/session:</strong> Suspicious</li>
<li><strong>&gt;50 pages:</strong> Likely bot</li>
<li><strong>&gt;200 pages:</strong> Definitely bot</li>
</ul>
<p><strong>Exception:</strong> Power users (journalists researching topic, students writing papers) might hit 20-30 pages. Combine with other signals.</p>
<h3>Time-of-Day and Frequency Patterns</h3>
<p><strong>Bots scrape 24/7.</strong> Humans have sleep cycles.</p>
<p><strong>Analysis:</strong></p>
<pre><code class="language-python">def check_24_7_activity(ip, requests):
    hours = [req[&#39;timestamp&#39;].hour for req in requests]
    hour_coverage = len(set(hours))

    if hour_coverage &gt;= 20:  # Active in 20+ hours/day
        return True  # Bot-like
    return False
</code></pre>
<p><strong>Frequency consistency:</strong></p>
<p>Bots maintain consistent daily volume. Humans vary day-to-day.</p>
<pre><code class="language-python">def check_consistent_daily_volume(ip, daily_counts):
    # daily_counts = [450, 445, 452, 448, 451, ...]
    if len(daily_counts) &lt; 7:
        return False

    stddev = statistics.stdev(daily_counts)
    mean = statistics.mean(daily_counts)

    if stddev / mean &lt; 0.15:  # Very low variation
        return True  # Bot
    return False
</code></pre>
<h2>Honeypot Detection</h2>
<h3>Trap Link Implementation</h3>
<p><strong>Concept:</strong> Embed hidden links in pages that humans never see but bots follow.</p>
<p><strong>Implementation:</strong></p>
<pre><code class="language-html">&lt;!-- Visible content --&gt;
&lt;article&gt;
  &lt;h1&gt;Article Title&lt;/h1&gt;
  &lt;p&gt;Article content...&lt;/p&gt;
&lt;/article&gt;

&lt;!-- Honeypot link (hidden from users) --&gt;
&lt;a href=&quot;/crawler-trap-do-not-access&quot; style=&quot;display:none; position:absolute; left:-9999px;&quot;&gt;.&lt;/a&gt;
</code></pre>
<p><strong>CSS ensures invisibility:</strong></p>
<pre><code class="language-css">.honeypot-link {
  display: none;
  visibility: hidden;
  position: absolute;
  left: -9999px;
  font-size: 0;
}
</code></pre>
<p><strong>robots.txt warning (ethical honeypot):</strong></p>
<pre><code>User-agent: *
Disallow: /crawler-trap-do-not-access
</code></pre>
<p><strong>Server-side handling:</strong></p>
<pre><code class="language-python">@app.route(&#39;/crawler-trap-do-not-access&#39;)
def honeypot():
    ip = request.remote_addr
    user_agent = request.headers.get(&#39;User-Agent&#39;)

    # Log trap access
    log_honeypot_trigger(ip, user_agent)

    # Alert team
    send_alert(f&quot;Honeypot triggered: {user_agent} from {ip}&quot;)

    # Add to blocklist
    add_to_blocklist(ip)

    return &quot;404 Not Found&quot;, 404  # Pretend page doesn&#39;t exist
</code></pre>
<h3>Behavioral Honeypots</h3>
<p><strong>Beyond hidden links:</strong> Trap URLs disguised as valuable content.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-html">&lt;a href=&quot;/premium-api-access-keys.txt&quot;&gt;API Keys&lt;/a&gt;
</code></pre>
<p><strong>Legitimate users wouldn&#39;t click.</strong> Bots scraping for credentials will.</p>
<p><strong>Another technique: Fake sitemap</strong></p>
<p>Create <code>sitemap-internal.xml</code> (not linked anywhere, disallowed in robots.txt) filled with trap URLs.</p>
<p>Bots ignoring robots.txt and aggressively seeking sitemaps will access it.</p>
<pre><code class="language-xml">&lt;urlset xmlns=&quot;http://www.sitemaps.org/schemas/sitemap/0.9&quot;&gt;
  &lt;url&gt;&lt;loc&gt;https://yoursite.com/trap-page-1&lt;/loc&gt;&lt;/url&gt;
  &lt;url&gt;&lt;loc&gt;https://yoursite.com/trap-page-2&lt;/loc&gt;&lt;/url&gt;
  ...
&lt;/urlset&gt;
</code></pre>
<p><strong>Any access to these URLs = bot caught.</strong></p>
<h3>Analyzing Honeypot Triggers</h3>
<p><strong>Honeypot access patterns reveal bot sophistication.</strong></p>
<p><strong>Pattern 1: Immediate trigger</strong></p>
<p>Bot accesses trap within seconds of visiting homepage → aggressive, indiscriminate scraping.</p>
<p><strong>Pattern 2: Delayed trigger</strong></p>
<p>Bot crawls site for days, eventually hits trap → systematic but robots.txt-ignoring.</p>
<p><strong>Pattern 3: Multiple traps triggered</strong></p>
<p>Bot hits 5+ different honeypots → comprehensive scraping operation.</p>
<p><strong>Response matrix:</strong></p>
<table>
<thead>
<tr>
<th>Trigger Speed</th>
<th>Traps Hit</th>
<th>Response</th>
</tr>
</thead>
<tbody><tr>
<td>Immediate</td>
<td>1</td>
<td>Block for 24h, monitor</td>
</tr>
<tr>
<td>Delayed</td>
<td>1</td>
<td>Log, investigate</td>
</tr>
<tr>
<td>Immediate</td>
<td>3+</td>
<td>Permanent block</td>
</tr>
<tr>
<td>Delayed</td>
<td>3+</td>
<td>Block, report to bot owner</td>
</tr>
</tbody></table>
<h2>Integrated Detection Framework</h2>
<h3>Multi-Signal Scoring System</h3>
<p><strong>Combine all detection methods into unified bot score.</strong></p>
<p><strong>Scoring logic:</strong></p>
<pre><code class="language-python">def calculate_bot_score(request_data):
    score = 0

    # User agent signals
    if matches_known_bot(request_data[&#39;user_agent&#39;]):
        score += 50
    if user_agent_spoofed(request_data[&#39;user_agent&#39;], request_data[&#39;headers&#39;]):
        score += 30

    # IP signals
    if not ip_verification_passes(request_data[&#39;ip&#39;], request_data[&#39;user_agent&#39;]):
        score += 40
    if geographic_anomaly(request_data[&#39;ip&#39;]):
        score += 20

    # Behavioral signals
    if sequential_scraping_detected(request_data[&#39;history&#39;]):
        score += 25
    if no_javascript_execution(request_data[&#39;ip&#39;]):
        score += 15
    if session_depth_excessive(request_data[&#39;ip&#39;]):
        score += 20

    # Honeypot
    if honeypot_triggered(request_data[&#39;ip&#39;]):
        score += 100

    return score

# Classification
score = calculate_bot_score(request)

if score &gt;= 100:
    classify_as_bot(high_confidence=True)
elif score &gt;= 50:
    classify_as_bot(medium_confidence=True)
else:
    classify_as_human()
</code></pre>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>0-49:</strong> Likely human</li>
<li><strong>50-99:</strong> Likely bot (moderate confidence)</li>
<li><strong>100+:</strong> Definitely bot (high confidence)</li>
</ul>
<h3>Real-Time Classification Pipeline</h3>
<p><strong>Architecture:</strong></p>
<ol>
<li><strong>Request arrives</strong> → web server</li>
<li><strong>Extract signals</strong> → IP, user agent, headers, referrer</li>
<li><strong>Lookup history</strong> → past requests from IP (Redis cache)</li>
<li><strong>Calculate score</strong> → multi-signal analysis</li>
<li><strong>Action decision</strong> → allow / rate-limit / block / challenge</li>
</ol>
<p><strong>Implementation (Nginx + Lua):</strong></p>
<pre><code class="language-lua">-- Nginx Lua module for real-time bot detection
local bot_score = 0

-- Check user agent
if ngx.var.http_user_agent:match(&quot;GPTBot&quot;) then
    bot_score = bot_score + 50
end

-- Check IP verification
local ip = ngx.var.remote_addr
if not verify_ip(ip, ngx.var.http_user_agent) then
    bot_score = bot_score + 40
end

-- Lookup session history (Redis)
local redis = require &quot;resty.redis&quot;
local red = redis:new()
red:connect(&quot;127.0.0.1&quot;, 6379)
local session_depth = red:get(&quot;session_depth:&quot; .. ip)

if tonumber(session_depth) &gt; 50 then
    bot_score = bot_score + 20
end

-- Decision
if bot_score &gt;= 100 then
    ngx.exit(ngx.HTTP_FORBIDDEN)
elseif bot_score &gt;= 50 then
    ngx.sleep(2)  -- Rate limit
end
</code></pre>
<h2>FAQ</h2>
<h3>What percentage of AI crawlers properly identify themselves?</h3>
<p><strong>Approximately 60-70% of AI training traffic</strong> uses identifiable user agents (GPTBot, ClaudeBot, etc.). Major AI companies (OpenAI, Anthropic, Google, Perplexity) announce crawlers for transparency and compliance. <strong>Remaining 30-40%</strong> comprises unlicensed scrapers, stealth crawlers, and aggregators using generic user agents or residential proxies. User agent detection alone misses significant scraping volume. Combine with IP verification and behavioral analysis to surface hidden activity.</p>
<h3>How accurate is IP range verification for detecting spoofed bots?</h3>
<p><strong>Highly accurate for major bots.</strong> OpenAI, Anthropic, Google publish official IP ranges. If user agent says &quot;GPTBot&quot; but IP isn&#39;t in published ranges, spoofing confidence &gt;95%. <strong>Limitations:</strong> (1) IP ranges change (companies add infrastructure), verify against updated lists quarterly; (2) Some bots use CDNs/proxies making IP verification complex; (3) DNS reverse lookup is alternative when IP ranges unavailable. <strong>Best practice:</strong> Use IP verification as primary filter, DNS lookup as fallback, behavioral analysis to catch sophisticated evasion.</p>
<h3>Can bots evade behavioral analysis by mimicking human patterns?</h3>
<p><strong>Yes, with significant effort.</strong> Advanced scrapers can randomize request intervals, vary session depths, execute JavaScript, fake referrers. <strong>But:</strong> Mimicking humans perfectly is difficult and computationally expensive. Most bots prioritize speed (scraping millions of pages quickly), which requires mechanical patterns detectable via behavioral analysis. <strong>Mitigation:</strong> Combine behavioral signals (no single signal is definitive), use honeypots (can&#39;t be evaded if scraper doesn&#39;t know they exist), implement challenge-response systems (CAPTCHA) for borderline cases.</p>
<h3>Should I block all traffic that fails bot detection tests?</h3>
<p><strong>No.</strong> False positives occur. Aggressive blocking risks excluding legitimate users, accessibility tools, research bots. <strong>Tiered response:</strong> (1) <strong>High-confidence bots (score 100+):</strong> Block or severe rate limit. (2) <strong>Medium confidence (50-99):</strong> Challenge with CAPTCHA or moderate rate limiting. (3) <strong>Low confidence (&lt;50):</strong> Monitor, log for analysis. <strong>Edge cases:</strong> Corporate proxy users, VPN users, power users researching your content might trigger false positives. Allow manual appeals or whitelist verified researchers.</p>
<h3>How often should I update my crawler detection database?</h3>
<p><strong>Weekly minimum for active monitoring, monthly acceptable for passive tracking.</strong> AI companies launch new crawlers quarterly. User agent strings evolve (version updates). IP ranges expand as infrastructure scales. <strong>Sources to monitor:</strong> (1) <a href="ai-crawler-directory-2026.html">ai-crawler-directory-2026.html</a> (updated weekly), (2) Bot developer documentation (check monthly), (3) Community GitHub repos (automated scraping). <strong>Automation:</strong> Script fetches updated lists, diffs against current database, alerts team to changes. Manual review before deploying updates (verify additions are legitimate, not malicious entries).</p>

      </article>

      <div class="cta-box" style="margin: var(--sp-16) 0;">
        <h3>Your Content Feeds AI. Get Paid for It.</h3>
        <p>Complete pay-per-crawl implementation. Templates, pricing models, licensing contracts. Everything.</p>
        <a href="/setup.html" class="btn btn--primary btn--large">Master the Protocol &mdash; $2,497</a>
      </div>

      <div style="margin-top: var(--sp-8); padding-top: var(--sp-8); border-top: 1px solid var(--border);">
        <a href="/articles.html" style="font-family: var(--font-mono); font-size: 0.875rem; font-weight: 500; color: var(--accent);">&larr; All Articles</a>
      </div>
    </div>
  </main>


  <footer class="footer">
    <div class="container">
      <div class="footer__grid">
        <div class="footer__brand">
          <div class="footer__logo">AI Pay Per Crawl</div>
          <p class="footer__desc">Your content feeds AI. The protocol ensures you get paid for it. Implementation guides, pricing models, and licensing infrastructure for publishers monetizing AI crawler traffic.</p>
          <a href="/setup.html" class="btn btn--primary" style="font-size: 0.8125rem; padding: 0.625rem 1.25rem;">Get Rule &mdash; $2,497</a>
        </div>

        <div>
          <div class="footer__heading">Implementation</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Cloudflare Setup</a></li>
            <li><a href="/articles.html">RSL Protocol</a></li>
            <li><a href="/articles.html">llms.txt Guide</a></li>
            <li><a href="/articles.html">Nginx Rules</a></li>
            <li><a href="/articles.html">Apache Config</a></li>
            <li><a href="/articles.html">WordPress Plugin</a></li>
            <li><a href="/articles.html">CDN Integration</a></li>
            <li><a href="/articles.html">robots.txt Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">Pricing &amp; Tools</div>
          <ul class="footer__links">
            <li><a href="/articles.html">Pricing Models</a></li>
            <li><a href="/articles.html">Rate Cards</a></li>
            <li><a href="/articles.html">Revenue Calculator</a></li>
            <li><a href="/articles.html">Crawler Directory</a></li>
            <li><a href="/articles.html">Bot Detection</a></li>
            <li><a href="/articles.html">Compliance Rates</a></li>
            <li><a href="/articles.html">Deal Analysis</a></li>
            <li><a href="/articles.html">Licensing Templates</a></li>
          </ul>
        </div>

        <div>
          <div class="footer__heading">From Scale With Search</div>
          <ul class="footer__links">
            <li><a href="https://scalewithsearch.com" target="_blank" rel="me">Scale With Search</a></li>
            <li><a href="https://aifirstsearch.com" target="_blank" rel="me">AI First Search</a></li>
            <li><a href="https://browserprompt.com" target="_blank" rel="me">Browser Prompt</a></li>
            <li><a href="https://polytraffic.com" target="_blank" rel="me">Polytraffic</a></li>
            <li><a href="https://creatinepedia.com" target="_blank" rel="me">Creatinepedia</a></li>
            <li><a href="https://victorvalentineromo.com" target="_blank" rel="me">Victor Romo</a></li>
            <li><a href="https://b2bvic.com" target="_blank" rel="me">B2B Vic</a></li>
          </ul>
        </div>
      </div>

      <div class="footer__bar">
        <p>&copy; 2026 AI Pay Per Crawl. Built by <a href="https://victorvalentineromo.com" target="_blank">Victor Valentine Romo</a>.</p>
        <p>A <a href="https://scalewithsearch.com" target="_blank">Scale With Search</a> property.</p>
      </div>
    </div>
  </footer>


  <script>
    /* Mega menu */
    (function() {
      var triggers = document.querySelectorAll('.mega-trigger');
      var panels = document.querySelectorAll('.mega-panel');
      var closeTimeout = null;

      function openPanel(id) {
        clearTimeout(closeTimeout);
        panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
        triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        var panel = document.getElementById('mega-' + id);
        var trigger = document.querySelector('[data-mega="' + id + '"] .mega-trigger');
        if (panel) panel.classList.add('mega-panel--open');
        if (trigger) trigger.setAttribute('aria-expanded', 'true');
      }

      function scheduleClose() {
        closeTimeout = setTimeout(function() {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }, 200);
      }

      function cancelClose() { clearTimeout(closeTimeout); }

      triggers.forEach(function(btn) {
        var wrapper = btn.closest('[data-mega]');
        var id = wrapper.getAttribute('data-mega');
        btn.addEventListener('mouseenter', function() { openPanel(id); });
        btn.addEventListener('click', function(e) {
          e.preventDefault();
          var panel = document.getElementById('mega-' + id);
          if (panel && panel.classList.contains('mega-panel--open')) {
            panel.classList.remove('mega-panel--open');
            btn.setAttribute('aria-expanded', 'false');
          } else {
            openPanel(id);
          }
        });
        wrapper.addEventListener('mouseleave', scheduleClose);
      });

      panels.forEach(function(panel) {
        panel.addEventListener('mouseenter', cancelClose);
        panel.addEventListener('mouseleave', scheduleClose);
      });

      document.addEventListener('click', function(e) {
        if (!e.target.closest('[data-mega]') && !e.target.closest('.mega-panel')) {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });

      document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape') {
          panels.forEach(function(p) { p.classList.remove('mega-panel--open'); });
          triggers.forEach(function(t) { t.setAttribute('aria-expanded', 'false'); });
        }
      });
    })();

    /* Mobile menu */
    var mobileMenuBtn = document.getElementById('mobile-menu-btn');
    var mobileMenu = document.getElementById('mobile-menu');
    var menuIcon = document.getElementById('menu-icon');
    var closeIcon = document.getElementById('close-icon');
    if (mobileMenuBtn) {
      mobileMenuBtn.addEventListener('click', function() {
        mobileMenu.classList.toggle('mobile-menu--open');
        menuIcon.classList.toggle('hidden');
        closeIcon.classList.toggle('hidden');
      });
    }

    /* Self-link neutralizer */
    (function() {
      var currentPath = window.location.pathname.replace(/\.html$/, '').replace(/\/$/, '') || '/';
      document.querySelectorAll('a[href]').forEach(function(a) {
        var href = a.getAttribute('href').replace(/\.html$/, '').replace(/\/$/, '') || '/';
        if (href === currentPath) {
          a.removeAttribute('href');
          a.setAttribute('aria-current', 'page');
          a.style.opacity = '0.5';
          a.style.pointerEvents = 'none';
        }
      });
    })();

    /* Copy code button */
    function copyCode(button) {
      var codeBlock = button.closest('.code-block');
      var code = codeBlock.querySelector('code').innerText;
      navigator.clipboard.writeText(code).then(function() {
        button.textContent = 'Copied!';
        setTimeout(function() { button.textContent = 'Copy'; }, 2000);
      });
    }

    /* FAQ Accordion */
    document.querySelectorAll('.accordion__trigger').forEach(function(trigger) {
      trigger.addEventListener('click', function() {
        var expanded = this.getAttribute('aria-expanded') === 'true';
        var body = this.nextElementSibling;
        this.setAttribute('aria-expanded', !expanded);
        body.classList.toggle('accordion__body--open');
      });
    });
  </script>
</body>
</html>